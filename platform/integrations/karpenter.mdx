---
title: Using Karpenter with vCluster Platform
sidebar_label: Karpenter
sidebar_position: 7
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import Flow, { Step } from "@site/src/components/Flow";
import Label from "@site/src/components/Label";

vCluster is compatible with Karpenter and vCluster's features can be used in conjuntion with Karpenter to
further reduce infrastructure costs.

This article will explain how to [Setup Designated Node Pool for a vCluster](#setup-designated-node-pool-for-a-vcluster), [Setup Designated Node
Pool for a Project](#setup-designated-node-pool-for-a-project), and [Setup Designated Node Pool for Dev/Prod Workloads Accross All vClusters](#setup-designated-node-pool-for-a-project). All
three of which can be powerful cost saving tools.

This article assumes you are using a host cluster provisioned on one of the following managed Kubernetes
services: EKS (Elastic Kubernetes Service) or AKS (Azure Kubernetes Service).

Much of this article borrows and builds upon the [Getting Started with Karpenter](https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/) documentation and the [Karpenter Provider Azure README.md](https://github.com/Azure/karpenter-provider-azure).

### Pre-requisites
1. Either an [EKS Kubernetes cluster with karpenter installed](https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/) or an [EKS Kubernetes cluster with karpenter installed](https://github.com/Azure/karpenter-provider-azure).
2. vCluster Platform installed on host cluster. Read [Deploy the vCluster Platform docs](https://www.vcluster.com/docs/platform/install/quick-start-guide#2-deploy-the-vcluster-platform).
    * The following permissions will be required in vCluster Platform to complete the integration instructions:
      * Create projects
      * Create vClusters
3. The following Kubernetes RBAC permissions for kubectl operations described in the integration instructions:

<Tabs
  defaultValue="EKS"
  values={[
    { label: "EKS RBAC Requirements", value: "EKS" },
    { label: "AKS RBAC Requirements", value: "AKS" },
  ]}
>
  <TabItem value="EKS">
    * Create `NodePools.karpenter.sh/v1beta1` in host cluster.
    * Create `EC2NodeClass.karpenter.sh/v1beta1` in host cluster.
    * Create `Deployments.apps/v1` in vCluster(s).
  </TabItem>
  <TabItem value="AKS">
    * Create `NodePools.karpenter.sh/v1beta1` in host cluster.
    * Create `AKSNodeClass.karpenter.sh/v1beta1` in host cluster.
    * Create `Deployments.apps/v1` in vCluster(s).
  </TabItem>
</Tabs>

4. EKS specific requirements for vCluster Platform compatibility:
    1. EBS driver installed. [Get the Amazon EBS CSI driver add-on](https://docs.aws.amazon.com/eks/latest/userguide/managing-ebs-csi.html#adding-ebs-csi-eks-add-on).
    2. Delete the gp2 StorageClass with `kubectl delete storageclass gp2` and replace with gp3 by applying following yaml:

```yaml

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp3
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: ebs.csi.aws.com
volumeBindingMode: WaitForFirstConsumer
parameters:
  type: gp3
allowVolumeExpansion: true

```

5. EKS specific requirements for integration instructions:
    1. You will need to know the node role created by karpenter. It should be of the form `KarpenterNodeRole-<eks-cluster-name>`

## Setup Designated Node Pool for a vCluster
This section utilizes taints and tolerations to guide virtual cluster pods to the desired node. This works best if every node pool or node utilizes a taint. Otherwise, the pods may be scheduled on non-tainted nodes.
1. Select "Create Virtual Cluster"
2. Select "Configure" and add a toleration to virtual cluster pods using the yaml editor. If the below yaml is out of date then see the [applying tolerations docs](https://www.vcluster.com/docs/v0.19/architecture/scheduling#automatically-applying-tolerations-to-all-pods-synced-by-vcluster).

```yaml
sync:
  toHost:
    pods:
      enforceTolerations:
      - vclusterID=<vcluster-name>:NoSchedule
```

3. Create a node pool with a taint matching the toleration and create node class for it to use.

<Tabs
  defaultValue="EKS"
  values={[
    { label: "Provision Node Pool for EKS", value: "EKS" },
    { label: "Provision Node Pool for AKS", value: "AKS" },
  ]}
>
  <TabItem value="EKS">

    ```bash
    cat <<EOF | envsubst | kubectl apply -f -
    ---
    apiVersion: karpenter.sh/v1beta1
    kind: NodePool
    metadata:
      name: <name-of-your-choosing>
    spec:
      template:
        spec:
          taints:
          - key: vclusterID
            value:  <vcluster-id>
            effect: NoSchedule
          requirements:
            - key: kubernetes.io/arch
              operator: In
              values: ["amd64"]
            - key: kubernetes.io/os
              operator: In
              values: ["linux"]
            - key: karpenter.sh/capacity-type
              operator: In
              values: ["spot"]
            - key: karpenter.k8s.aws/instance-category
              operator: In
              values: ["c", "m", "r"]
            - key: karpenter.k8s.aws/instance-generation
              operator: Gt
              values: ["2"]
          nodeClassRef:
            apiVersion: karpenter.k8s.aws/v1beta1
            kind: EC2NodeClass
            name: <node-class-name>
      limits:
        cpu: 100
      disruption:
        consolidationPolicy: WhenUnderutilized
        expireAfter: Never
    ---
    apiVersion: karpenter.k8s.aws/v1beta1
    kind: EC2NodeClass
    metadata:
      name: <name-of-your-choosing>
    spec:
      amiFamily: AL2 # Amazon Linux 2
      role: "<karpenter-node-role>" # replace with your cluster name
      subnetSelectorTerms:
        - tags:
            karpenter.sh/discovery: "<cluster-name>"
      securityGroupSelectorTerms:
        - tags:
            karpenter.sh/discovery: "<cluster-name>"
      amiSelectorTerms:
        - id: "<arm-ami-id>"
        - id: "<amd-ami-id>"
EOF

      ```

  </TabItem>
  <TabItem value="AKS">

    ```bash
    cat <<EOF | kubectl apply -f -
    ---
    apiVersion: karpenter.sh/v1beta1
    kind: NodePool
    metadata:
      name: <name-of-your-choosing>
    spec:
      template:
        spec:
          taints:
          - key: vclusterID
            value:  <cluster-id>
            effect: NoSchedule
          requirements:
            - key: kubernetes.io/arch
              operator: In
              values: ["amd64"]
            - key: kubernetes.io/os
              operator: In
              values: ["linux"]
            - key: karpenter.sh/capacity-type
              operator: In
              values: ["on-demand"]
            - key: karpenter.azure.com/sku-family
              operator: In
              values: [D]
          nodeClassRef:
            apiVersion: karpenter.k8s.aws/v1beta1
            kind: AKSNodeClass
            name: <node-class-name>
      limits:
        cpu: 100
      disruption:
        consolidationPolicy: WhenUnderutilized
        expireAfter: Never
    ---
    apiVersion: karpenter.azure.com/v1alpha2
    kind: AKSNodeClass
    metadata:
      name: <name-of-your-choosing>
    spec:
      imageFamily: Ubuntu2204
EOF
    ```
  </TabItem>
</Tabs>

4. Now we will confirm the node pool is working. Run `kubectl get nodes --watch`. Open a separate terminal tab. Connect to your vCluster by running `vcluster connect <vcluster-id>`. Then run:
```bash
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inflate
spec:
  replicas: 0
  selector:
    matchLabels:
      app: inflate
  template:
    metadata:
      labels:
        app: inflate
    spec:
      terminationGracePeriodSeconds: 0
      containers:
        - name: inflate
          image: public.ecr.aws/eks-distro/kubernetes/pause:3.7
          resources:
            requests:
              cpu: 1
EOF

kubectl scale deployment inflate --replicas 5

```

You should see nodes provision for the deployment. If you do the above on a vCluster without the tolerations then the nodes should not be provisioned on the node pool.

5. Tear down the deployment used above.
```bash
kubectl delete deployment inflate

```

## Create Designated Node Pool for a Project
We will designate a node pool for a project by creating templates and enforcing the template for that project.
1. Using the Vcluster Platform add a template: Templates -> Virtual Clusters -> Add Virtual Cluster Template.
2. In the yaml editor box, which contains the vcluster.yaml, add the following:
```yaml
sync:
  toHost:
    pods:
      enforceTolerations:
      - vclusterID=<project-name>:NoSchedule
```
3. Create a node pool with the taint corresponding to the toleration in step 2, e.g.:

<Tabs
  defaultValue="EKS"
  values={[
    { label: "Provision NodePool for EKS", value: "EKS" },
    { label: "Provision NodePool for AKS", value: "AKS" },
  ]}
>
  <TabItem value="EKS">

    ```bash
    cat <<EOF | envsubst | kubectl apply -f -
    apiVersion: karpenter.sh/v1beta1
    kind: NodePool
    metadata:
      name: <name-of-your-choosing>
    spec:
      template:
        spec:
          taints:
          - key: <project-name>
            value: "true"
            effect: NoSchedule
          requirements:
            - key: kubernetes.io/arch
              operator: In
              values: ["amd64"]
            - key: kubernetes.io/os
              operator: In
              values: ["linux"]
            - key: karpenter.sh/capacity-type
              operator: In
              values: ["spot"]
            - key: karpenter.k8s.aws/instance-category
              operator: In
              values: ["c", "m", "r"]
            - key: karpenter.k8s.aws/instance-generation
              operator: Gt
              values: ["2"]
          nodeClassRef:
            apiVersion: karpenter.k8s.aws/v1beta1
            kind: EC2NodeClass
            name: <node-class-name>
      limits:
        cpu: 100
      disruption:
        consolidationPolicy: WhenUnderutilized
        expireAfter: Never
EOF
```

  </TabItem>
  <TabItem value="AKS">

    ```bash
    cat <<EOF | envsubst | kubectl apply -f -
    apiVersion: karpenter.sh/v1beta1
    kind: NodePool
    metadata:
      name: <name-of-your-choosing>
    spec:
      template:
        spec:
          taints:
          - key: <project-name>
            value: "true"
            effect: NoSchedule
          requirements:
            - key: kubernetes.io/arch
              operator: In
              values: ["amd64"]
            - key: kubernetes.io/os
              operator: In
              values: ["linux"]
            - key: karpenter.sh/capacity-type
              operator: In
              values: ["on-demand"]
            - key: karpenter.azure.com/sku-family
              operator: In
              values: [D]
          nodeClassRef:
            apiVersion: karpenter.k8s.aws/v1beta1
            kind: AKSNodeClass
            name: <node-class-name>
      limits:
        cpu: 100
      disruption:
        consolidationPolicy: WhenUnderutilized
        expireAfter: Never
EOF
    ```

  </TabItem>
</Tabs>

4. Create a new project that only allows template(s) with set tolerations. Go to project dropdown and create a new project. In the Allowed Templates section, remove the All Templates option. Select the template(s) that include tolerations for the node pool created in step 3. Now, all pods created in virtual clusters belonging to this project will deploy to the designated node pool(s).

### Target Node Pool Based on vCluster Pod Labels
1. Create a node pool with custom label(s) e.g. `my.org/environ`.

<Tabs
  defaultValue="EKS"
  values={[
    { label: "Provision NodePool for EKS", value: "EKS" },
    { label: "Provision NodePool for AKS", value: "AKS" },
  ]}
>
  <TabItem value="EKS">

    ```bash
    cat <<EOF | envsubst | kubectl apply -f -
    apiVersion: karpenter.sh/v1beta1
    kind: NodePool
    metadata:
      name: default
    spec:
      template:
        spec:
          requirements:
            - key: kubernetes.io/arch
              operator: In
              values: ["amd64"]
            - key: kubernetes.io/os
              operator: In
              values: ["linux"]
            - key: karpenter.sh/capacity-type
              operator: In
              values: ["spot"]
            - key: karpenter.k8s.aws/instance-category
              operator: In
              values: ["c", "m", "r"]
            - key: karpenter.k8s.aws/instance-generation
              operator: Gt
              values: ["2"]
            - key: my.org/environ
              operator: Exists
          nodeClassRef:
            apiVersion: karpenter.k8s.aws/v1beta1
            kind: EC2NodeClass
            name: <node-class-name>
      limits:
        cpu: 100
      disruption:
        consolidationPolicy: WhenUnderutilized
        expireAfter: Never
EOF
    ```

  </TabItem>
  <TabItem value="AKS">

    ```bash
    cat <<EOF | envsubst | kubectl apply -f -
    apiVersion: karpenter.sh/v1beta1
    kind: NodePool
    metadata:
      name: default
    spec:
      template:
        spec:
          requirements:
            - key: kubernetes.io/arch
              operator: In
              values: ["amd64"]
            - key: kubernetes.io/os
              operator: In
              values: ["linux"]
            - key: karpenter.sh/capacity-type
              operator: In
              values: ["on-demand"]
            - key: karpenter.azure.com/sku-family
              operator: In
              values: [D]
            - key: my.org/environ
              operator: Exists
          nodeClassRef:
            apiVersion: karpenter.k8s.aws/v1beta1
            kind: AKSNodeClass
            name: <node-class-name>
      limits:
        cpu: 100
      disruption:
        consolidationPolicy: WhenUnderutilized
        expireAfter: Never
EOF
    ```

  </TabItem>
</Tabs>

2. Add nodeSelector to deployments in your virtual cluster using a nodeSelector that targets the label from step 1 with the desired value.

```bash
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inflate
spec:
  replicas: 0
  selector:
    matchLabels:
      app: inflate
  template:
    metadata:
      labels:
        app: inflate
    spec:
      nodeSelector:
        my.org/environ: <any key, "prod", "dev", "test" make sense here>
      terminationGracePeriodSeconds: 0
      containers:
        - name: inflate
          image: public.ecr.aws/eks-distro/kubernetes/pause:3.7
          resources:
            requests:
              cpu: 1
EOF

kubectl scale deployment inflate --replicas 5

```

If you make a deployment with a key/value pair that matches an existing node it can be scheduled to that node. Otherwise, a new node will be provisioned. Since you are adding the key to the node pool but not the value, using different values necessitates different nodes. You can read more about user-defined labels in the [Karpenter's User-Defined Labels docs](https://karpenter.sh/docs/concepts/scheduling/#user-defined-labels).
