---
title: StatefulSet
sidebar_label: statefulSet
sidebar_position: 3
description: Configuration for ...
---

import ControlPlaneStatefulSet from '../../../../_partials/config/controlPlane/statefulSet.mdx'

The vCluster control plane is deployed as a StatefulSet, since vCluster requires a PersistentVolume resource to store data across restarts. When you configure `statefulSet.persistence.volumeClaim.enabled: false` or you don't configure `statefulSet.persistence.volumeClaimTemplates`,  vCluster is deployed as a `Deployment`.

## High availability

- The `highAvailability` settings let you run more than one pod for the vCluster control plane, with one running as the leader.
- If the leader crashes, is unhealthy, or restarts, more pods take over leadership, depending on the number of replicas.
- Adjusting the leaseDuration / renewDeadline / retryPeriod changes the leader election behavior (how often a leader is renewed & retries when it cannot be renewed)

## Scheduling

`scheduling` configures various scheduler behavior for different purposes. Examples of each follow:
- `nodeSelector`: Matches labels on nodes to make the scheduler "prefer" scheduling the virtual cluster pods on certain nodes. Do this to:
  - Target nodes in a region.
  - Target a specific architecture or machine class.
- `affinity`: Value can be affinity (attract the pod) or anti-affinity (repel the pod).
  - Use anti-affinity to spread pods away from each other. A common technique is to make virtual cluster pods repel each other so that they are not scheduled on the same nodes. This increases resiliency in the event a node is scaled down or replaced by a cloud provider.
  - Use affinity to group certain pods together to reduce network latency for critical services
- `tolerations`: Another method of influencing where the scheduler places pods. A common use is to "taint" nodes for non-virtual-cluster workloads and make virtual cluster workloads "tolerate" the taint. This results in separating critical apps from the more ephemeral vCluster instances.
- `priorityClassName`: See [Pod Priority and Preemption](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/).
- `podManagementPolicy`: See [Pod Management Policies](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies).
- `topologySpreadConstraints`: See [Pod Topology Spread Constraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/).
  - Similar to podAffinity & podAntiAffinity in some ways. See [Comparison with podAffinity and podAntiAffinity](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#comparison-with-podaffinity-podantiaffinity).

## Re-using existing PersistenceVolumeClaim
It is possible to pre-provision a PersistenceVolumeClaim, and then configure vCluster to use it.
To do that, set existing claim name in the chart values `controlPlane.statefulSet.persistence.dataVolume` under `persistenceVolumeClaim.claimName`.

```yaml
controlPlane:
  statefulSet:
    persistence:
      dataVolume:
        - name: data
          persistentVolumeClaim:
            claimName: my-existing-pvc
```

## vCluster image options

There are currently 3 vCluster image builds you can use in `statefulSet.image.repository`: 

- loft-sh/vcluster-pro: The default image for the Helm chart, this image works for all use-cases.
- loft-sh/vcluster-oss: A purely open-source build of vCluster
- loft-sh/vcluster: Deprecated. Use `loft-sh/vcluster-oss` as a replacement.

## Config reference

<ControlPlaneStatefulSet/>
