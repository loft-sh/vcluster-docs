
import PrivateNodes from '../_partials/config/privateNodes.mdx'
import AutoUpgrade from '../_partials/config/privateNodes/autoUpgrade.mdx'
import Networking from '../_partials/config/networking.mdx'
import ControlPlane from '../_partials/config/controlPlane.mdx'
import ProAdmonition from '../_partials/admonitions/pro-admonition.mdx'

<ProAdmonition/>

Using private nodes is another use case for vCluster, where instead of taking advantage of sharing worker nodes of a host cluster, you dedicate worker nodes to be bound to a single vCluster. In this use case, the control plane still runs as a pod in a host cluster and there is no ability to use the worker nodes of the host cluster. Instead, dedicated worker nodes need to be added to the vCluster before being able to deploy resources. Since the dedicated worker nodes are real nodes, there is no need to "sync" resources to the physical node as that node is treated as the virtual cluster's worker node. Another way to think about this use case is that the control plane runs as a pod on a host cluster creating a virtual cluster context, but the workers nodes are private physical nodes in that virtual cluster context. One of the benefits of adding private nodes to a vCluster is that vCluster can automatically manage the lifecycle of the worker node.

To allow nodes to join the virtual cluster, the virtual cluster needs to be exposed and accessible. One option is to expose a LoadBalancer use this endpoint for the worker nodes. vCluster does not need to reach the nodes directly and instead uses [konnectivity](https://kubernetes.io/docs/tasks/extend-kubernetes/setup-konnectivity/) to tunnel requests from the vCluster to the nodes (e.g. for `kubectl logs` etc.).

<br />

<center>
<img
src="/docs/media/private-nodes/architecture.png"
width='500'
alt="Overview"
/>
</center>

## Comparison Private Nodes vs Resource Syncing

For an overview of what is possible with private nodes mode vs regular resource syncing, see the following table:

<br />

|  | Private Nodes | Resource Syncing |
|---|---|---|
| Control Plane as Pods | Yes | Yes |
| Isolated Control Plane | Yes | Yes |
| Install Custom CNI | Yes | No |
| Install Custom CSI Drivers | Yes | No |
| Separate Network | Yes | No |
| Completely Isolated Nodes | Yes | No |
| Reuse Host Nodes | No | Yes |
| Reuse Host Controllers | No | Yes |
| See vCluster Resources in Host Cluster | No | Yes |

## vcluster.yaml behavior

### Enable Scheduling

By enabling private nodes, scheduling must now be owned by the virtual cluster, and therefore, the virtual scheduler is automatically enabled. This yaml doesn't need to be included in your `vcluster.yaml`. 

```yaml
# Enabling private nodes enables the virtual scheduler
# Disabling the virtual scheduler is not possible
controlPlane:
  advanced:
    virtualScheduler:
      enabled: true

```

### Other Feature Limitations

When private nodes are enabled, the following `vcluster.yaml` options are no longer supported.

```yaml
# No syncing is supported
sync:

# No integration is supported as integrations rely on syncing
integrations:

# No services will be replicated
networking:
  replicateServices:


controlPlane:
  distro:
    # k3s distro is not supported, only k8s is supported
    k3s:

  # Embedded coreDNS is not supported as changing CNI options is one of the benefits of private nodes
  coredns:
    embedded: true
      
  advanced:
    # Disabling the virtual scheduler is not possible
    virtualScheduler:
      enabled: false
```

## Create vCluster with Private Nodes

To create a virtual cluster with private nodes, it's as simple as enabling the feature. 

```yaml
# Enable private nodes
privateNodes:
  enabled: true

# vCluster control plane options
controlPlane:
  distro:
    k8s:
      image:
        tag: v1.31.2 # Kubernetes version you want to use
  service:
    spec:
      type: LoadBalancer # If you want to expose vCluster via LoadBalancer (recommended option)

# Networking configuration
networking:
  # Specify the pod cidr
  podCIDR: 10.64.0.0/16
  # Specify the service cidr
  serviceCIDR: 10.128.0.0/16
```

:::info Connecting to the Platform
Follow [this guide](https://www.vcluster.com/docs/vcluster/next/configure/vcluster-yaml/external/platform/api-key#connect-virtual-clusters-to-the-vcluster-platform) to create an access key and setup the vCluster for deployment via Helm or other tools.
:::

## Joining Worker Nodes into the vCluster

vCluster currently supports two modes of joining nodes into it:
* Automatically via [cluster api](https://github.com/kubernetes-sigs/cluster-api)
* Manually via [kubeadm](https://kubernetes.io/docs/reference/setup-tools/kubeadm/)

### Join Worker Node via Kubeadm

Before you can join actual nodes into the vCluster, first you need to create a token for the nodes to join. You can either use a single token for all nodes or create a token per node, this is up to you.

To create a new token, simply run the following command while connected to the vCluster:
```bash
vcluster token create
```

This will print a command similar to:
```bash
curl -sfLk https://<vcluster-endpoint>/node/join?token=<token> | sh -
```

Simply run the command to join a node into the vCluster:
```bash
Preparing node for Kubernetes installation...
Kubernetes version: v1.30.1
Installing Kubernetes binaries...
Enabling containerd and kubelet...
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /etc/systemd/system/containerd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /etc/systemd/system/kubelet.service.
Starting containerd and kubelet...
Installation successful!
Joining node into cluster...
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

:::info Using vanilla kubeadm
If you want to see the raw kubeadm join command you can use the `--kubeadm` flag instead.
:::

### Join Worker Nodes via Cluster API

For automatically joining nodes via Cluster API, please setup the cluster via the following command to install the [vCluster Cluster API provider](https://github.com/loft-sh/cluster-api-provider-vcluster):
```yaml
clusterctl init --infrastructure vcluster
```

As soon as the cluster-api provider is running, you can install a provider to do the actual node creation, e.g. KubeVirt.
```bash
VCLUSTER_VERSION=v0.26.0-alpha.9

clusterctl generate cluster test --kubernetes-version v1.31.2 -n test --from https://raw.githubusercontent.com/loft-sh/cluster-api-provider-vcluster/refs/heads/main/hack/kubevirt/template.yaml
```

Then apply the manifests to the cluster and Cluster API should create the vCluster and join the worker nodes via the KubeVirt provider.

## vCluster Control Plane upgrades

:::warning Kubernetes Version Skew Policy applies
Keep in mind to not upgrade multiple minor versions at the same time for the control plane as outlined in the [Kubernetes Version Skew Policy](https://kubernetes.io/releases/version-skew-policy/). Instead always upgrade a single minor version, wait until the cluster becomes healthy and then upgrade to the next minor version. E.g. v1.26 -> v1.27 -> v1.28
:::

Upgrading the vCluster control plane is super simple and only requires exchanging the Kubernetes init container version, vCluster will then automatically upgrade the control plane and if configured automatically upgrade all the worker nodes. To exchange the Kubernetes version, specify the following in your `vcluster.yaml`:
```yaml
...
controlPlane:
  statefulSet:
    image:
      tag: v1.31.1 # or any other kubernetes version
...
```

## vCluster Worker upgrades

:::warning Kubernetes Version Skew Policy applies
Keep in mind to follow the [Kubernetes Version Skew Policy](https://kubernetes.io/releases/version-skew-policy/) for worker upgrades.
:::

There is two modes of how worker upgrades can be done:
* Automatically by vCluster when the control-plane was updated (recommended)
* Manually via vCluster CLI or kubeadm

### Automatic Upgrades by vCluster

Automatic worker node upgrades are enabled by default and vCluster will try to upgrade nodes as soon as it detects a mismatch between control-plane and worker nodes. By default vCluster will upgrade a single node at a time.

The upgrade pod will run directly on the node and do the following steps on the node:
1. Download the Kubernetes binary bundle from the vCluster control plane
2. Exchange kubeadm binary
3. Run kubeadm upgrade node
4. Cordon the node
5. Exchange other binaries such as containerd, kubelet etc.
6. Restart containerd and kubelet if needed
7. Uncordon the node

You can also skip specific nodes from the automatic update by adding the label `vcluster.loft.sh/skip-auto-upgrade=true` on the node or specifying a `nodeSelector` via the vCluster config.

<AutoUpgrade />

:::warning 
Node upgrades usually shouldn't restart any pods, but depending on the Kubernetes version upgrade it might be possible.
:::

### Manual Upgrades

You can either upgrade a node via vCluster CLI by running the following command:
```bash
vcluster node upgrade my-node
```

Or by manually using the [Kubeadm Kubernetes guide](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/) from the official documentation. 

## Addons

### CNI

vCluster will install [Flannel](https://github.com/flannel-io/flannel) by default, if you want to install your own custom CNI, you can disable that via:
```yaml
privateNodes:
  cni:
    flannel:
      enabled: false
```

### CoreDNS

vCluster will install [CoreDNS](https://coredns.io/) by default to allow cluster dns resolving of services and pods.

### Konnectivity

vCluster uses [Konnectivity](https://kubernetes.io/docs/tasks/extend-kubernetes/setup-konnectivity/) to establish a connection between control-plane and worker nodes for commands like `kubectl logs`, `kubectl exec`, `kubectl port-forward` etc. The Konnectivity server runs as part of the vCluster control plane, while an agent is deployed inside the vCluster. If not needed or wanted, you can disable Konnectivity in the vCluster config.

### Kube Proxy

vCluster installs [Kube Proxy](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/) by default to ensure that services are configured on the nodes. Some CNI implement their own kube-proxy functionality, so you can optionally disable kube-proxy deployment in the vCluster config.

### Local Path Provisioner

vCluster installs [Local Path Provisioner](https://github.com/rancher/local-path-provisioner) by default to allow stateful workloads within the vCluster.

## Load Docker Image into a Node

To load a local docker image into a worker node you can use the following command:
```bash
vcluster node load-image my-node --image nginx:latest
```

## Config reference

<PrivateNodes />
<Networking />
<ControlPlane />
