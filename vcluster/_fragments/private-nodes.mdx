import PrivateNodes from '../_partials/config/privateNodes.mdx'
import AutoUpgrade from '../_partials/config/privateNodes/autoUpgrade.mdx'
import Networking from '../_partials/config/networking.mdx'
import ControlPlane from '../_partials/config/controlPlane.mdx'
import ProAdmonition from '../_partials/admonitions/pro-admonition.mdx'

<ProAdmonition/>

## Use private nodes

Using private nodes is a deployment mode for vCluster where, instead of sharing the host cluster’s worker nodes, you assign dedicated worker nodes to a single vCluster instance.

In this setup, the control plane runs as a Pod in the host cluster, but host cluster worker nodes are not used for scheduling workloads. Instead, you must attach dedicated physical nodes to the vCluster. These private nodes act as the vCluster’s worker nodes and are treated as part of the virtual cluster environment.

Because the dedicated nodes are real Kubernetes nodes, vCluster does not need to sync workloads to the host. All workloads run directly on the attached nodes as if they were native to the virtual cluster.

This approach can be understood as follows: the control plane runs in the host cluster, creating a virtual Kubernetes context, while the workloads run on separate physical nodes that exist entirely within that virtual context.

One key benefit of using private nodes is that vCluster can automatically manage the lifecycle of the worker nodes.

To allow nodes to join the virtual cluster, the cluster must be exposed and accessible. One option is to expose a `LoadBalancer` service and use its endpoint for the worker nodes. vCluster does not need direct access to the nodes; instead, it uses [Konnectivity](https://kubernetes.io/docs/tasks/extend-kubernetes/setup-konnectivity/) to tunnel requests—such as `kubectl logs`—from the control plane to the nodes.


<br />

<center>
<img
src="/docs/media/private-nodes/architecture.png"
width='500'
alt="Overview"
/>
</center>

<!-- vale off -->

## Compare private nodes and resource syncing

The following table outlines the differences between using private nodes and using resource syncing: 

| Feature                            | Private Nodes | Resource Syncing |
|------------------------------------|---------------|------------------|
| Control plane runs as Pods         | Yes           | Yes              |
| Isolated control plane             | Yes           | Yes              |
| Install custom CNI                 | Yes           | No               |
| Install custom CSI drivers         | Yes           | No               |
| Separate network                   | Yes           | No               |
| Completely isolated nodes          | Yes           | No               |
| Reuse host nodes                   | No            | Yes              |
| Reuse host controllers             | No            | Yes              |
| View vCluster resources in host    | No            | Yes              |

## Configure vcluster.yaml behavior

### Enable scheduling

When private nodes are enabled, scheduling is handled by the virtual cluster. As a result, the virtual scheduler is automatically enabled. You do not need to manually configure this in your `vcluster.yaml`.

```yaml title="Virtual scheduler automatically enabled with private nodes"
# Enabling private nodes enables the virtual scheduler
# Disabling the virtual scheduler is not possible
controlPlane:
  advanced:
    virtualScheduler:
      enabled: true
```

### Understand feature limitations

When private nodes are enabled, the following `vcluster.yaml` options are no longer supported.

```yaml title="Unsupported vcluster.yaml options with private nodes"
# No syncing is supported
sync:

# No integration is supported as integrations rely on syncing
integrations:

# No services are replicated
networking:
  replicateServices:

controlPlane:
  distro:
    # k3s distro is not supported, only k8s is supported
    k3s:

  # Embedded coreDNS is not supported as changing CNI options is one of the benefits of private nodes
  coredns:
    embedded: true
      
  advanced:
    # Disabling the virtual scheduler is not possible
    virtualScheduler:
      enabled: false
```

## Create vCluster with private nodes

To create a virtual cluster with private nodes, enable the feature:

```yaml title="Enable private nodes in vCluster configuration"
# Enable private nodes
privateNodes:
  enabled: true

# vCluster control plane options
controlPlane:
  distro:
    k8s:
      image:
        tag: v1.31.2 # Kubernetes version you want to use
  service:
    spec:
      type: LoadBalancer # If you want to expose vCluster via LoadBalancer (recommended option)

# Networking configuration
networking:
  # Specify the pod cidr
  podCIDR: 10.64.0.0/16
  # Specify the service cidr
  serviceCIDR: 10.128.0.0/16
```

:::info Connecting to the platform
Follow [this guide](https://www.vcluster.com/docs/vcluster/next/configure/vcluster-yaml/external/platform/api-key#connect-virtual-clusters-to-the-vcluster-platform) to create an access key and set up the vCluster for deployment using Helm or other tools.
:::

## Join worker nodes to the vCluster

vCluster currently supports two modes of joining nodes into it:
- Automatically using the [Cluster API](https://github.com/kubernetes-sigs/cluster-api)
- Manually using [`kubeadm`](https://kubernetes.io/docs/reference/setup-tools/kubeadm/)

### Join worker nodes using kubeadm

Before you can join actual nodes into the vCluster, you must create a token for the nodes to join. You can either use a single token for all nodes or create a token for each node.

To create a new token, run the following command while connected to the vCluster:

```bash title="Create a join token for worker nodes"
vcluster token create
```

The output is similar to the following:
```bash title="Example join command output"
curl -sfLk https://<vcluster-endpoint>/node/join?token=<token> | sh -
```

Run the command to join a node into the vCluster:

```bash title="Example successful node join output"
Preparing node for Kubernetes installation...
Kubernetes version: v1.30.1
Installing Kubernetes binaries...
Enabling containerd and kubelet...
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /etc/systemd/system/containerd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /etc/systemd/system/kubelet.service.
Starting containerd and kubelet...
Installation successful!
Joining node into cluster...
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

:::info Using vanilla kubeadm
To view the raw `kubeadm join` command, use the `--kubeadm` flag.
:::

### Join worker nodes using the Cluster API

For automatically joining nodes using the Cluster API, set up the cluster using the following command to install the [vCluster Cluster API provider](https://github.com/loft-sh/cluster-api-provider-vcluster):

```bash title="Initialize Cluster API with vCluster provider"
clusterctl init --infrastructure vcluster
```

As soon as the Cluster API provider is running, you can install a provider to handle node creation—for example, KubeVirt.

```bash title="Generate cluster configuration with KubeVirt"
VCLUSTER_VERSION=v0.26.0-alpha.9

clusterctl generate cluster test \
  --kubernetes-version v1.31.2 \
  -n test \
  --from https://raw.githubusercontent.com/loft-sh/cluster-api-provider-vcluster/refs/heads/main/hack/kubevirt/template.yaml
```

Then apply the manifests to the cluster and Cluster API should create the vCluster and join the worker nodes using the KubeVirt provider.

## Upgrade vCluster control plane

:::warning Kubernetes Version Skew Policy applies
Do not upgrade multiple minor versions at the same time for the control plane as outlined in the [Kubernetes Version Skew Policy](https://kubernetes.io/releases/version-skew-policy/). Instead always upgrade a single minor version, wait until the cluster becomes healthy and then upgrade to the next minor version. For example: `v1.26` -> `v1.27` -> `v1.28`.
:::

To upgrade the vCluster control plane, update the Kubernetes `init` container version in your `vcluster.yaml` file.
After the change, vCluster upgrades the control plane automatically. If automatic worker node upgrades are configured, those nodes are also upgraded.

To change the Kubernetes version, add the following to your `vcluster.yaml`:

```yaml title="Upgrade control plane Kubernetes version"
...
controlPlane:
  statefulSet:
    image:
      tag: v1.31.1 # Or any other Kubernetes version
...
```

## Upgrade vCluster workers

:::warning Kubernetes Version Skew Policy applies
Use the [Kubernetes Version Skew Policy](https://kubernetes.io/releases/version-skew-policy/) for worker upgrades.
:::

There are two modes of how worker upgrades can be done:
- (_Recommended_) Automatically by vCluster when the control-plane was updated
- Manually using vCluster CLI or kubeadm

### Use automatic upgrades

Automatic worker node upgrades are enabled by default. vCluster upgrades each node when it detects a version mismatch between the control plane and the worker node. By default, one node is upgraded at a time.

The upgrade Pod runs directly on the node and completes the following steps:
1. Download the Kubernetes binary bundle from the vCluster control plane
2. Replace the `kubeadm` binary
3. Run `kubeadm upgrade node`
4. Cordon the node
5. Replace other binaries such as `containerd`, `kubelet`, etc.
6. Restart `containerd` and `kubelet` if necessary
7. Uncordon the node

You can also skip specific nodes from the automatic update by adding the label `vcluster.loft.sh/skip-auto-upgrade=true` on the node or specifying a `nodeSelector` via the vCluster config.

<AutoUpgrade />

:::warning 
Node upgrades typically do not restart pods. However, depending on the Kubernetes version change, restarts might occur in some cases.
:::

### Perform manual upgrades

You can either upgrade a node using the vCluster CLI by running the following command:

```bash title="Upgrade a specific node manually"
vcluster node upgrade my-node
```

Alternatively, you can manually upgrade by following the [official Kubeadm Kubernetes guide](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/).

## Configure addons

vCluster supports addons that extend the functionality of your virtual cluster. You can configure these addons during deployment to adjust networking, observability, and other features for your environment and requirements.

### CNI

vCluster installs [Flannel](https://github.com/flannel-io/flannel) by default, if you want to install your own custom CNI, you can disable Flannel using the following:

```yaml title="Disable default Flannel CNI"
privateNodes:
  cni:
    flannel:
      enabled: false
```

### CoreDNS

vCluster installs [CoreDNS](https://coredns.io/) by default to allow cluster DNS resolving of services and pods.

### Konnectivity

vCluster uses [Konnectivity](https://kubernetes.io/docs/tasks/extend-kubernetes/setup-konnectivity/) to establish a connection between the control plane and worker nodes for commands such as `kubectl logs`, `kubectl exec`, `kubectl port-forward`. The Konnectivity server runs as part of the vCluster control plane, while an agent is deployed inside the vCluster. If it is not needed or wanted, you can disable Konnectivity in the vCluster config.

### Kube proxy

vCluster installs [Kube Proxy](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/) by default to ensure that services are configured on the nodes. Some CNI implement their own kube-proxy functionality. Optionally, you can disable kube-proxy deployment in the vCluster config.

### Local Path Provisioner

vCluster installs the [Local Path Provisioner](https://github.com/rancher/local-path-provisioner) by default to allow stateful workloads within the vCluster.

## Load Docker images into nodes

To load a local Docker image into a worker node you can use the following command:

```bash title="Load Docker image into specific node"
vcluster node load-image my-node --image nginx:latest
```

## Review configuration reference

<PrivateNodes />
<Networking />
<ControlPlane />
