import PrivateNodes from '../_partials/config/privateNodes.mdx'
import AutoUpgrade from '../_partials/config/privateNodes/autoUpgrade.mdx'
import Networking from '../_partials/config/networking.mdx'
import ControlPlane from '../_partials/config/controlPlane.mdx'
import ProAdmonition from '../_partials/admonitions/pro-admonition.mdx'

<ProAdmonition/>

## Use private nodes

Using private nodes is a deployment mode for vCluster where, instead of sharing the host cluster’s worker nodes, you assign dedicated worker nodes to a single vCluster instance.

In this setup, the control plane runs as a Pod in the host cluster, but host cluster worker nodes are not used for scheduling workloads. Instead, you must attach dedicated physical nodes to the vCluster. These private nodes act as the vCluster’s worker nodes and are treated as part of the virtual cluster environment.

Because the dedicated nodes are real Kubernetes nodes, vCluster does not need to sync workloads to the host. All workloads run directly on the attached nodes as if they were native to the virtual cluster.

This approach can be understood as follows: the control plane runs in the host cluster, creating a virtual Kubernetes context, while the workloads run on separate physical nodes that exist entirely within that virtual context.

One key benefit of using private nodes is that vCluster can automatically manage the lifecycle of the worker nodes.

To allow nodes to join the virtual cluster, the cluster must be exposed and accessible. One option is to expose a `LoadBalancer` service and use its endpoint for the worker nodes. vCluster does not need direct access to the nodes; instead, it uses [Konnectivity](https://kubernetes.io/docs/tasks/extend-kubernetes/setup-konnectivity/) to tunnel requests—such as `kubectl logs`—from the control plane to the nodes.


<br />

<center>
<img
src="/docs/media/private-nodes/architecture.png"
width='500'
alt="Overview"
/>
</center>

<!-- vale off -->

## Private nodes vs. shared nodes

The following table outlines the differences between using private nodes and shared nodes: 

| Feature                                    | Private Nodes                | Shared Nodes                 |
|--------------------------------------------|------------------------------|------------------------------|
| Control plane runs as pods                 | Yes                          | Yes                          |
| Isolated control plane                     | Yes                          | Yes                          |
| Ability to install custom CNI              | Yes                          | No                           |
| Ability to install custom CSI drivers      | Yes                          | No                           |
| Isolated network                           | Yes                          | No                           |
| Worker Nodes                               | Isolated to the vCluster     | Shared from the host cluster |
| Ability to re-use host controllers         | No                           | Yes                          |
| Ability to view vCluster resources in host | No                           | Yes                          |

## vcluster.yaml configuration

To create a vCluster with private nodes, you'll need to enable the feature as well as set some other configuration options in preparation for worker nodes to join.

```yaml title="Enable private nodes"
# Enable private nodes
privateNodes:
  enabled: true

# vCluster control plane options
controlPlane:
  distro:
    k8s:
      image:
        tag: v1.31.2 # Kubernetes version you want to use
  service:
    spec:
      type: LoadBalancer # If you want to expose vCluster using LoadBalancer (Recommended option)

# Networking configuration
networking:
  # Specify the pod CIDR
  podCIDR: 10.64.0.0/16
  # Specify the service CIDR
  serviceCIDR: 10.128.0.0/16
```

### Scheduling enabled by default

When private nodes are enabled, scheduling is handled by the virtual cluster. As a result, the virtual scheduler is automatically enabled. You do not need to manually configure this in your `vcluster.yaml`.

```yaml title="Virtual scheduler automatically enabled with private nodes"
# Enabling private nodes enables the virtual scheduler
# Disabling the virtual scheduler is not possible
controlPlane:
  advanced:
    virtualScheduler:
      enabled: true
```

### Configure add-ons

vCluster supports add-ons that extends the functionality of your virtual cluster. You can configure these add-ons during deployment to adjust networking, observability, and other features for your environment and requirements.

#### CNI

By default, vCluster installs [flannel](https://github.com/flannel-io/flannel) as the CNI. You can install your own CNI by disabling flannel.

```yaml title="Disable default flannel CNI "
privateNodes:
  cni:
    flannel:
      enabled: false
```

#### CoreDNS

vCluster installs [CoreDNS](https://coredns.io/) by default to allow cluster DNS resolving of services and pods.

#### Konnectivity

vCluster uses [Konnectivity](https://kubernetes.io/docs/tasks/extend-kubernetes/setup-konnectivity/) to establish a connection between the control plane and worker nodes for commands such as `kubectl logs`, `kubectl exec`, `kubectl port-forward`. The Konnectivity server runs as part of the vCluster control plane, while an agent is deployed inside the vCluster. If it is not needed or wanted, you can disable Konnectivity in the vCluster config.

#### Kube proxy

vCluster installs [Kube Proxy](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/) by default to ensure that services are configured on the nodes. Some CNI implement their own kube-proxy functionality. Optionally, you can disable kube-proxy deployment in the vCluster config.

#### Local Path Provisioner

vCluster installs the [Local Path Provisioner](https://github.com/rancher/local-path-provisioner) by default to allow stateful workloads within the vCluster.

### Other vCluster feature limitations

When private nodes are enabled, certain vCluster features are automatically disabled or unavailable. If you include these options in your `vcluster.yaml`, they are ignored or might cause configuration errors.

The following features are not available when using private nodes:

- `sync.*` - No resource syncing between virtual and host clusters
- `integrations.*` - Integrations depend on syncing functionality  
- `networking.replicateServices` - Services are not replicated to host
- `controlPlane.distro.k3s` - Only standard Kubernetes (k8s) is supported
- `controlPlane.coredns.embedded: true` - Embedded CoreDNS conflicts with custom CNI
- `controlPlane.advanced.virtualScheduler.enabled: false` - Virtual scheduler cannot 
- `sleepMode.*` - No ability to sleep workloads or control plane. 

```yaml title="Unsupported vcluster.yaml options with private nodes"
# These configurations are NOT supported with private nodes

# Resource syncing between virtual and host clusters is disabled
sync:
  services:
    enabled: false  # Services cannot be synced to host cluster
  secrets:
    enabled: false  # Secrets cannot be synced to host cluster
                    # All other sync options (pods, configmaps, etc.) are also disabled

# Platform integrations require syncing functionality
integrations:
  metricsServer:
    enabled: false  # Metrics server integration not supported
                    # All other integrations are disabled due to sync dependency

# Service replication to host cluster is not available
networking:
  replicateServices:
    enabled: false  # Services run entirely within virtual cluster

# Distribution restrictions
controlPlane:
  distro:
    k3s:
      enabled: false  # k3s distribution not supported
    k8s:
      enabled: true   # Only standard Kubernetes works

  # DNS configuration limitations
  coredns:
    embedded: false   # Embedded CoreDNS conflicts with custom CNI options
  advanced:
    # Virtual scheduler is required for workload placement
    virtualScheduler:
      enabled: true   # Always enabled (cannot be disabled)

# Sleep mode is not available
sleepMode: 
  enabled: false
```

## Create vCluster and manually add private nodes

### Prerequisites

- vCluster CLI installed on your local machine
- Access to a host cluster
- Worker nodes 

### Create the vCluster control plane

Before joining private nodes to a vCluster, you need to create vCluster to deploy the control plane on a host cluster.

```yaml title="vcluster.yaml for private nodes"
# Enable private nodes
privateNodes:
  enabled: true

# vCluster control plane options
controlPlane:
  distro:
    k8s:
      image:
        tag: v1.31.2 # Kubernetes version you want to use
  service:
    spec:
      type: LoadBalancer # If you want to expose vCluster via LoadBalancer (recommended option)

# Networking configuration
networking:
  # Specify the pod cidr
  podCIDR: 10.64.0.0/16
  # Specify the service cidr
  serviceCIDR: 10.128.0.0/16
```

:::info Connecting to the platform
Follow [this guide](https://www.vcluster.com/docs/vcluster/next/configure/vcluster-yaml/external/platform/api-key#connect-virtual-clusters-to-the-vcluster-platform) to create an access key and set up the vCluster for deployment using Helm or other tools.
:::

### Join worker nodes to the virtual cluster

:::info Prerequisite
Worker nodes must have `curl` installed before they can join the vCluster.
:::

To join worker nodes, a token from the vCluster must be created to provide access and permissions to join the vCluster. A single token can be used for any worker nodes to join, or if you wanted to, you could create a token for each node. 

```bash title="Create a token for worker nodes"
export VCLUSTER_NAME=my-vcluster

# Connect to your vcluster
vcluster connect $VCLUSTER_NAME

# Create a token
vcluster token create
```

The output provides a command to run on your worker node:

```bash title="Example output from creating a token"
curl -sfLk https://<vcluster-endpoint>/node/join?token=<token> | sh -
```

For each worker node that you want to join vCluster, run the command on the worker node.

```bash title="Example output on worker node"
Preparing node for Kubernetes installation...
Kubernetes version: v1.31.2
Installing Kubernetes binaries...
Enabling containerd and kubelet...
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /etc/systemd/system/containerd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /etc/systemd/system/kubelet.service.
Starting containerd and kubelet...
Installation successful!
Joining node into cluster...
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

## Create vCluster and worker nodes using CAPI 

For automatically creating virtual cluster and private nodes using the [Cluster API](https://github.com/kubernetes-sigs/cluster-api), set up the cluster using the following command to install the [vCluster Cluster API provider](https://github.com/loft-sh/cluster-api-provider-vcluster):

```bash title="Initialize Cluster API with vCluster provider"
clusterctl init --infrastructure vcluster
```

As soon as the Cluster API provider is running, you can install a provider to handle node creation—for example, KubeVirt.

```bash title="Generate cluster configuration with KubeVirt"
VCLUSTER_VERSION=v0.26.0-alpha.9

clusterctl generate cluster test \
  --kubernetes-version v1.31.2 \
  -n test \
  --from https://raw.githubusercontent.com/loft-sh/cluster-api-provider-vcluster/refs/heads/main/hack/kubevirt/template.yaml
```

Then apply the manifests to the cluster and Cluster API should create the vCluster and join the worker nodes using the KubeVirt provider.

## Load Docker images 

To load a local Docker image on to a worker node you can use the following command:

```bash title="Load Docker image into specific node"
vcluster node load-image my-node --image nginx:latest
```

## Upgrade vCluster 

### Upgrade vCluster control plane

:::warning Kubernetes Version Skew Policy applies
Do not upgrade multiple minor versions at the same time for the control plane as outlined in the [Kubernetes Version Skew Policy](https://kubernetes.io/releases/version-skew-policy/). Instead always upgrade a single minor version, wait until the cluster becomes healthy and then upgrade to the next minor version. For example: `v1.26` -> `v1.27` -> `v1.28`.
:::

To upgrade the vCluster control plane, update the Kubernetes `init` container version in your `vcluster.yaml` file.
After the change, vCluster upgrades the control plane automatically. If automatic worker node upgrades are configured, those nodes are also upgraded.

To change the Kubernetes version, add the following to your `vcluster.yaml`:

```yaml title="Upgrade control plane Kubernetes version"
...
controlPlane:
  statefulSet:
    image:
      tag: v1.31.1 # Or any other Kubernetes version
...
```

### Upgrade vCluster workers

:::warning Kubernetes Version Skew Policy applies
Use the [Kubernetes Version Skew Policy](https://kubernetes.io/releases/version-skew-policy/) for worker upgrades.
:::

There are two modes of how worker upgrades can be done:
- (_Recommended_) Automatically by vCluster when the control-plane was updated
- Manually using vCluster CLI or kubeadm

#### Use automatic upgrades

Automatic worker node upgrades are enabled by default. vCluster upgrades each node when it detects a version mismatch between the control plane and the worker node. By default, one node is upgraded at a time.

The upgrade Pod runs directly on the node and completes the following steps:
1. Download the Kubernetes binary bundle from the vCluster control plane
2. Replace the `kubeadm` binary
3. Run `kubeadm upgrade node`
4. Cordon the node
5. Replace other binaries such as `containerd`, `kubelet`, etc.
6. Restart `containerd` and `kubelet` if necessary
7. Uncordon the node

You can also skip specific nodes from the automatic update by adding the label `vcluster.loft.sh/skip-auto-upgrade=true` on the node or specifying a `nodeSelector` via the vCluster config.

<AutoUpgrade />

:::warning
Node upgrades typically do not restart pods. However, depending on the Kubernetes version change, restarts might occur in some cases.
:::

#### Perform manual upgrades

You can either upgrade a node using the vCluster CLI by running the following command:

```bash title="Upgrade a specific node manually"
vcluster node upgrade my-node
```

Alternatively, you can manually upgrade by following the [official Kubeadm Kubernetes guide](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/).

## Config reference

<PrivateNodes />
<Networking />
<ControlPlane />
