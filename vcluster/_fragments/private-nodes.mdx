
import PrivateNodes from '../_partials/config/privateNodes.mdx'
import Networking from '../_partials/config/networking.mdx'
import ControlPlane from '../_partials/config/controlPlane.mdx'
import ProAdmonition from '../_partials/admonitions/pro-admonition.mdx'

<ProAdmonition/>

Enabling the private nodes feature in vCluster will turn off syncing of resources and instead transform vCluster into a regular hosted control plane you can join nodes into. In this case, the control plane will still run in a host cluster, but actual worker nodes need to be joined into the vCluster either via kubeadm or cluster-api. As soon as nodes are joined into the vCluster, vCluster can automatically upgrade the nodes and take over the lifecycle of them.

To allow actual nodes to join the vCluster control plane, the vCluster can be exposed by a LoadBalancer and this endpoint is used for joining worker nodes. Worker nodes must only be able to reach each other and the exposed vCluster, vCluster does not need to reach the nodes directly and instead uses [konnectivity](https://kubernetes.io/docs/tasks/extend-kubernetes/setup-konnectivity/) to tunnel requests from the vCluster to the nodes (e.g. for `kubectl logs` etc.). vCluster automatically deploys the Konnectivity service into the cluster.

<br />

<center>
<img
src="/docs/media/private-nodes/architecture.png"
width='500'
alt="Overview"
/>
</center>

## Create vCluster with Private Nodes

To start vCluster in private nodes mode use the following configuration:
```yaml
# This enables private node mode
privateNodes:
  enabled: true

# Control Plane options
controlPlane:
  distro:
    k8s:
      image:
        tag: v1.31.2 # Kubernetes version you want to use
  service:
    spec:
      type: LoadBalancer # If you want to expose vCluster via LoadBalancer (recommended option)

# Networking configuration
networking:
  # Specify the pod cidr
  podCIDR: 10.64.0.0/16
  # Specify the service cidr
  serviceCIDR: 10.128.0.0/16
```

Enabling this mode will currently disable the following other configurations:
* All syncing of resources to the host cluster will not work anymore
* The scheduler is enabled by default
* K3s distro is not supported for this mode
* All integrations in the `vcluster.yaml` are disabled
* Embedded CoreDNS is not supported
* Replicated services is not supported

:::info Deploy without the Platform
Follow [this guide](https://www.vcluster.com/docs/vcluster/next/configure/vcluster-yaml/external/platform/api-key#connect-virtual-clusters-to-the-vcluster-platform) to create an access key and setup the vCluster for deployment via Helm or other tools.
:::

## Joining Worker Nodes into the vCluster

vCluster currently supports two modes of joining nodes into it:
* Automatically via [cluster api](https://github.com/kubernetes-sigs/cluster-api)
* Manually via [kubeadm](https://kubernetes.io/docs/reference/setup-tools/kubeadm/)

### Join Worker Node via Kubeadm

Before you can join actual nodes into the vCluster, first you need to create a token for the nodes to join. You can either use a single token for all nodes or create a token per node, this is up to you.

To create a new token, simply run the following command while connected to the vCluster:
```bash
vcluster token create
```

This will print a command similar to:
```bash
kubeadm join --token <token> <control-plane-host>:<control-plane-port> --discovery-token-ca-cert-hash sha256:<hash>
```

Since kubeadm does not have an easy way to install kube binaries or a container runtime, we also offer a convenience script to do the setup for you:
```bash
# The kubernetes version of the vcluster
KUBERNETES_VERSION=v1.32.1

# Install required tools (containerd, kubelet etc.) and prepare node
curl -sfL https://raw.githubusercontent.com/loft-sh/init-node/main/init.sh | sh -s -- --kubernetes-version $KUBERNETES_VERSION
```

After the installation is complete, simply run the above command to join a node into the vCluster:
```bash
# Command obtained via running 'vcluster token create'
$ kubeadm join --token <token> <control-plane-host>:<control-plane-port> --discovery-token-ca-cert-hash sha256:<hash>
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```

### Join Worker Nodes via Cluster API

For automatically joining nodes via Cluster API, please setup the cluster via:
```yaml
clusterctl init --infrastructure vcluster
```

As soon as the cluster-api provider is running, you can install a provider to do the actual node creation, e.g. KubeVirt.
```bash
VCLUSTER_VERSION=v0.26.0-alpha.9

clusterctl generate cluster test --kubernetes-version v1.31.2 -n test --from https://raw.githubusercontent.com/loft-sh/cluster-api-provider-vcluster/refs/heads/main/hack/kubevirt/template.yaml
```

Then apply the manifests to the cluster and Cluster API should create the vCluster and join the worker nodes via the KubeVirt provider.

## vCluster Control Plane upgrades

:::warning Kubernetes Version Skew Policy applies
Keep in mind to not upgrade multiple minor versions at the same time for the control plane as outlined in the [Kubernetes Version Skew Policy](https://kubernetes.io/releases/version-skew-policy/). Instead always upgrade a single minor version, wait until the cluster becomes healthy and then upgrade to the next minor version. E.g. v1.26 -> v1.27 -> v1.28
:::

Upgrading the vCluster control plane is super simple and only requires exchanging the Kubernetes init container version, vCluster will then automatically upgrade the control plane and if configured automatically upgrade all the worker nodes. To exchange the Kubernetes version, specify the following in your `vcluster.yaml`:
```yaml
...
controlPlane:
  statefulSet:
    image:
      tag: v1.31.1 # or any other kubernetes version
...
```

## vCluster Worker upgrades

There is two modes of how worker upgrades can be done:
* Automatically by vCluster when the control-plane was updated
* Manually via vCluster CLI or kubeadm

### Automatic Upgrades by vCluster

Automatic worker nodes upgrades are enabled by default and vCluster will try to upgrade nodes as soon as it detects a mismatch between control-plane and worker nodes.

The upgrade pod will run directly on the node and do the following steps on the node:
1. Download the binary bundle from https://github.com/loft-sh/kubernetes/releases
2. Exchange kubeadm binary
3. Run kubeadm upgrade node
4. Cordon the node
5. Exchange other binaries such as containerd, kubelet etc.
6. Restart containerd and kubelet if needed
7. Uncordon the node

### Manual Kubeadm Upgrades

Please follow the guide in the [Kubernetes documentation](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/). 

## vCluster Private Nodes Mode without a Hosted Cluster

Coming soon...

## Config reference

<PrivateNodes />
<Networking />
<ControlPlane />
