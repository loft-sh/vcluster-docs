
import PrivateNodes from '../_partials/config/privateNodes.mdx'
import AutoUpgrade from '../_partials/config/privateNodes/autoUpgrade.mdx'
import Networking from '../_partials/config/networking.mdx'
import ControlPlane from '../_partials/config/controlPlane.mdx'
import ProAdmonition from '../_partials/admonitions/pro-admonition.mdx'

<ProAdmonition/>

Enabling the private nodes feature in vCluster will turn off syncing of resources and instead transform vCluster into a regular hosted control plane you can join regular worker nodes into. In this case, the control plane will still run as a pod in a host cluster, but actual worker nodes need to be joined into the vCluster for it to be able to schedule workloads to. As soon as nodes are joined into the vCluster, vCluster can automatically upgrade the nodes and take over the lifecycle of them.

To allow actual nodes to join the vCluster control plane, the vCluster can be exposed by a LoadBalancer and this endpoint is used for joining worker nodes. Worker nodes must be able to reach the exposed vCluster. vCluster does not need to reach the nodes directly and instead uses [konnectivity](https://kubernetes.io/docs/tasks/extend-kubernetes/setup-konnectivity/) to tunnel requests from the vCluster to the nodes (e.g. for `kubectl logs` etc.).

<br />

<center>
<img
src="/docs/media/private-nodes/architecture.png"
width='500'
alt="Overview"
/>
</center>

## Comparison Private Nodes vs Resource Syncing

For an overview of what is possible with private nodes mode vs regular resource syncing, see the following table:

<br />

|  | Private Nodes | Resource Syncing |
|---|---|---|
| Control Plane as Pods | Yes | Yes |
| Isolated Control Plane | Yes | Yes |
| Install Custom CNI | Yes | No |
| Install Custom CSI Drivers | Yes | No |
| Separate Network | Yes | No |
| Completely Isolated Nodes | Yes | No |
| Reuse Host Nodes | No | Yes |
| Reuse Host Controllers | No | Yes |
| See vCluster Resources in Host Cluster | No | Yes |

## Create vCluster with Private Nodes

To start vCluster in private nodes mode use the following configuration:
```yaml
# This enables private node mode
privateNodes:
  enabled: true

# Control Plane options
controlPlane:
  distro:
    k8s:
      image:
        tag: v1.31.2 # Kubernetes version you want to use
  service:
    spec:
      type: LoadBalancer # If you want to expose vCluster via LoadBalancer (recommended option)

# Networking configuration
networking:
  # Specify the pod cidr
  podCIDR: 10.64.0.0/16
  # Specify the service cidr
  serviceCIDR: 10.128.0.0/16
```

Enabling this mode will currently disable the following other configurations:
* All syncing of resources to the host cluster will not work anymore
* The scheduler is enabled by default
* K3s distro is not supported for this mode
* All integrations in the `vcluster.yaml` are disabled
* Embedded CoreDNS is not supported
* Replicated services is not supported

:::info Deploy without the Platform
Follow [this guide](https://www.vcluster.com/docs/vcluster/next/configure/vcluster-yaml/external/platform/api-key#connect-virtual-clusters-to-the-vcluster-platform) to create an access key and setup the vCluster for deployment via Helm or other tools.
:::

## Joining Worker Nodes into the vCluster

vCluster currently supports two modes of joining nodes into it:
* Automatically via [cluster api](https://github.com/kubernetes-sigs/cluster-api)
* Manually via [kubeadm](https://kubernetes.io/docs/reference/setup-tools/kubeadm/)

### Join Worker Node via Kubeadm

Before you can join actual nodes into the vCluster, first you need to create a token for the nodes to join. You can either use a single token for all nodes or create a token per node, this is up to you.

To create a new token, simply run the following command while connected to the vCluster:
```bash
vcluster token create
```

This will print a command similar to:
```bash
curl -sfLk https://<vcluster-endpoint>/node/join?token=<token> | sh -
```

Simply run the command to join a node into the vCluster:
```bash
reparing node for Kubernetes installation...
Kubernetes version: v1.30.1
Installing Kubernetes binaries...
Enabling containerd and kubelet...
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /etc/systemd/system/containerd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /etc/systemd/system/kubelet.service.
Starting containerd and kubelet...
Installation successful!
Joining node into cluster...
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

:::info Using vanilla kubeadm
If you want to see the raw kubeadm join command you can use the `--kubeadm` flag instead.
:::

### Join Worker Nodes via Cluster API

For automatically joining nodes via Cluster API, please setup the cluster via the following command to install the [vCluster Cluster API provider](https://github.com/loft-sh/cluster-api-provider-vcluster):
```yaml
clusterctl init --infrastructure vcluster
```

As soon as the cluster-api provider is running, you can install a provider to do the actual node creation, e.g. KubeVirt.
```bash
VCLUSTER_VERSION=v0.26.0-alpha.9

clusterctl generate cluster test --kubernetes-version v1.31.2 -n test --from https://raw.githubusercontent.com/loft-sh/cluster-api-provider-vcluster/refs/heads/main/hack/kubevirt/template.yaml
```

Then apply the manifests to the cluster and Cluster API should create the vCluster and join the worker nodes via the KubeVirt provider.

## vCluster Control Plane upgrades

:::warning Kubernetes Version Skew Policy applies
Keep in mind to not upgrade multiple minor versions at the same time for the control plane as outlined in the [Kubernetes Version Skew Policy](https://kubernetes.io/releases/version-skew-policy/). Instead always upgrade a single minor version, wait until the cluster becomes healthy and then upgrade to the next minor version. E.g. v1.26 -> v1.27 -> v1.28
:::

Upgrading the vCluster control plane is super simple and only requires exchanging the Kubernetes init container version, vCluster will then automatically upgrade the control plane and if configured automatically upgrade all the worker nodes. To exchange the Kubernetes version, specify the following in your `vcluster.yaml`:
```yaml
...
controlPlane:
  statefulSet:
    image:
      tag: v1.31.1 # or any other kubernetes version
...
```

## vCluster Worker upgrades

:::warning Kubernetes Version Skew Policy applies
Keep in mind to follow the [Kubernetes Version Skew Policy](https://kubernetes.io/releases/version-skew-policy/) for worker upgrades.
:::

There is two modes of how worker upgrades can be done:
* Automatically by vCluster when the control-plane was updated (recommended)
* Manually via vCluster CLI or kubeadm

### Automatic Upgrades by vCluster

Automatic worker node upgrades are enabled by default and vCluster will try to upgrade nodes as soon as it detects a mismatch between control-plane and worker nodes. By default vCluster will upgrade a single node at a time.

The upgrade pod will run directly on the node and do the following steps on the node:
1. Download the Kubernetes binary bundle from the vCluster control plane
2. Exchange kubeadm binary
3. Run kubeadm upgrade node
4. Cordon the node
5. Exchange other binaries such as containerd, kubelet etc.
6. Restart containerd and kubelet if needed
7. Uncordon the node

You can also skip specific nodes from the automatic update by adding the label `vcluster.loft.sh/skip-auto-upgrade=true` on the node or specifying a `nodeSelector` via the vCluster config.

<AutoUpgrade />

:::warning 
Node upgrades usually shouldn't restart any pods, but depending on the Kubernetes version upgrade it might be possible.
:::

### Manual Upgrades

You can either upgrade a node via vCluster CLI by running the following command:
```bash
vcluster node upgrade my-node
```

Or by manually using the [Kubeadm Kubernetes guide](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/) from the official documentation. 

## Addons

### CNI

vCluster will install [Flannel](https://github.com/flannel-io/flannel) by default, if you want to install your own custom CNI, you can disable that via:
```yaml
privateNodes:
  cni:
    flannel:
      enabled: false
```

### CoreDNS

vCluster will install [CoreDNS](https://coredns.io/) by default to allow cluster dns resolving of services and pods.

### Konnectivity

vCluster uses [Konnectivity](https://kubernetes.io/docs/tasks/extend-kubernetes/setup-konnectivity/) to establish a connection between control-plane and worker nodes for commands like `kubectl logs`, `kubectl exec`, `kubectl port-forward` etc. The Konnectivity server runs as part of the vCluster control plane, while an agent is deployed inside the vCluster. If not needed or wanted, you can disable Konnectivity in the vCluster config.

### Kube Proxy

vCluster installs [Kube Proxy](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/) by default to ensure that services are configured on the nodes. Some CNI implement their own kube-proxy functionality, so you can optionally disable kube-proxy deployment in the vCluster config.

### Local Path Provisioner

vCluster installs [Local Path Provisioner](https://github.com/rancher/local-path-provisioner) by default to allow stateful workloads within the vCluster.

## Load Docker Image into a Node

To load a local docker image into a worker node you can use the following command:
```bash
vcluster node load-image my-node --image nginx:latest
```

## Config reference

<PrivateNodes />
<Networking />
<ControlPlane />
