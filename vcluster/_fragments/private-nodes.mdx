import PrivateNodes from '../_partials/config/privateNodes.mdx'
import AutoUpgrade from '../_partials/config/privateNodes/autoUpgrade.mdx'
import Networking from '../_partials/config/networking.mdx'
import ControlPlane from '../_partials/config/controlPlane.mdx'

## Use private nodes

Using private nodes is a deployment mode for vCluster where, instead of sharing the host cluster’s worker nodes, you assign dedicated worker nodes to a single vCluster instance.

In this setup, the control plane runs as a Pod in the host cluster, but host cluster worker nodes are not used for scheduling workloads. Instead, you must attach dedicated physical nodes to the vCluster. These private nodes act as the vCluster’s worker nodes and are treated as part of the virtual cluster environment.

Because the dedicated nodes are real Kubernetes nodes, vCluster does not need to sync workloads to the host. All workloads run directly on the attached nodes as if they were native to the virtual cluster.

This approach can be understood as follows: the control plane runs in the host cluster, creating a virtual Kubernetes context, while the workloads run on separate physical nodes that exist entirely within that virtual context.

One key benefit of using private nodes is that vCluster can automatically manage the lifecycle of the worker nodes.

To allow nodes to join the virtual cluster, the cluster must be exposed and accessible. One option is to expose a `LoadBalancer` service and use its endpoint for the worker nodes. vCluster does not need direct access to the nodes; instead, it uses [Konnectivity](https://kubernetes.io/docs/tasks/extend-kubernetes/setup-konnectivity/) to tunnel requests—such as `kubectl logs`—from the control plane to the nodes.


<br />

<center>
<img
src="/docs/media/private-nodes/architecture.png"
width='500'
alt="Overview"
/>
</center>

<!-- vale off -->

## Private nodes vs. shared nodes

The following table outlines the differences between using private nodes and shared nodes:

| Feature                                    | Private Nodes                | Shared Nodes                 |
|--------------------------------------------|------------------------------|------------------------------|
| Control plane runs as pods                 | Yes                          | Yes                          |
| Isolated control plane                     | Yes                          | Yes                          |
| Ability to install custom CNI              | Yes                          | No                           |
| Ability to install custom CSI drivers      | Yes                          | No                           |
| Isolated network                           | Yes                          | No                           |
| Worker Nodes                               | Isolated to the vCluster     | Shared from the host cluster |
| Ability to re-use host controllers         | No                           | Yes                          |
| Ability to view vCluster resources in host | No                           | Yes                          |

## vcluster.yaml configuration

To create a vCluster with private nodes, you'll need to enable the feature as well as set some other configuration options in preparation for worker nodes to join.

```yaml title="Enable private nodes"
# Enable private nodes
privateNodes:
  enabled: true

# vCluster control plane options
controlPlane:
  distro:
    k8s:
      image:
        tag: v1.31.2 # Kubernetes version you want to use
  service:
    spec:
      type: LoadBalancer # If you want to expose vCluster using LoadBalancer (Recommended option)

# Networking configuration
networking:
  # Specify the pod CIDR
  podCIDR: 10.64.0.0/16
  # Specify the service CIDR
  serviceCIDR: 10.128.0.0/16
```

### Scheduling enabled by default

When private nodes are enabled, scheduling is handled by the virtual cluster. As a result, the virtual scheduler is automatically enabled. You do not need to manually configure this in your `vcluster.yaml`.

```yaml title="Virtual scheduler automatically enabled with private nodes"
# Enabling private nodes enables the virtual scheduler
# Disabling the virtual scheduler is not possible
controlPlane:
  advanced:
    virtualScheduler:
      enabled: true
```

### Configure add-ons

vCluster supports add-ons that extends the functionality of your virtual cluster. You can configure these add-ons during deployment to adjust networking, observability, and other features for your environment and requirements.

#### CNI

By default, vCluster installs [flannel](https://github.com/flannel-io/flannel) as the CNI. You can install your own CNI by disabling flannel.

```yaml title="Disable default flannel CNI "
privateNodes:
  cni:
    flannel:
      enabled: false
```

#### CoreDNS

vCluster installs [CoreDNS](https://coredns.io/) by default to allow cluster DNS resolving of services and pods.

#### Konnectivity

vCluster uses [Konnectivity](https://kubernetes.io/docs/tasks/extend-kubernetes/setup-konnectivity/) to establish a connection between the control plane and worker nodes for commands such as `kubectl logs`, `kubectl exec`, `kubectl port-forward`. The Konnectivity server runs as part of the vCluster control plane, while an agent is deployed inside the vCluster. If it is not needed or wanted, you can disable Konnectivity in the vCluster config.

#### Kube proxy

vCluster installs [Kube Proxy](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/) by default to ensure that services are configured on the nodes. Some CNI implement their own kube-proxy functionality. Optionally, you can disable kube-proxy deployment in the vCluster config.

#### Local Path Provisioner

vCluster installs the [Local Path Provisioner](https://github.com/rancher/local-path-provisioner) by default to allow stateful workloads within the vCluster.

### Other vCluster feature limitations

When private nodes are enabled, certain vCluster features are automatically disabled or unavailable. If you include these options in your `vcluster.yaml`, they are ignored or might cause configuration errors.

The following features are not available when using private nodes:

- `sync.*` - No resource syncing between virtual and host clusters
- `integrations.*` - Integrations depend on syncing functionality
- `networking.replicateServices` - Services are not replicated to host
- `controlPlane.distro.k3s` - Only standard Kubernetes (k8s) is supported
- `controlPlane.coredns.embedded: true` - Embedded CoreDNS conflicts with custom CNI
- `controlPlane.advanced.virtualScheduler.enabled: false` - Virtual scheduler cannot
- `sleepMode.*` - No ability to sleep workloads or control plane.

```yaml title="Unsupported vcluster.yaml options with private nodes"
# These configurations are NOT supported with private nodes

# Resource syncing between virtual and host clusters is disabled
sync:
  services:
    enabled: false  # Services cannot be synced to host cluster
  secrets:
    enabled: false  # Secrets cannot be synced to host cluster
                    # All other sync options (pods, configmaps, etc.) are also disabled

# Platform integrations require syncing functionality
integrations:
  metricsServer:
    enabled: false  # Metrics server integration not supported
                    # All other integrations are disabled due to sync dependency

# Service replication to host cluster is not available
networking:
  replicateServices:
    enabled: false  # Services run entirely within virtual cluster

# Distribution restrictions
controlPlane:
  distro:
    k3s:
      enabled: false  # k3s distribution not supported
    k8s:
      enabled: true   # Only standard Kubernetes works

  # DNS configuration limitations
  coredns:
    embedded: false   # Embedded CoreDNS conflicts with custom CNI options
  advanced:
    # Virtual scheduler is required for workload placement
    virtualScheduler:
      enabled: true   # Always enabled (cannot be disabled)
  # Host Path Mapper is not supported
  hostPathMapper:
    enabled: false

# Sleep mode is not available
sleepMode:
  enabled: false

# Isolated Control Plane mode is not supported
experimental:
  isolatedControlPlane:
    enabled: false
```

## Create vCluster and manually add private nodes

### OS Prerequisites

- Linux distribution with SystemD support (required for service management)
- `iptables` binaries installed (automatically available on Ubuntu, may need manual installation on other distributions)
- `curl` installed
- Root user access

### Node Prerequisites

- vCluster CLI installed on your local machine
- Access to a host cluster
- Worker nodes

### Create the vCluster control plane

Before joining private nodes to a vCluster, you need to create vCluster to deploy the control plane on a host cluster.

```yaml title="vcluster.yaml for private nodes"
# Enable private nodes
privateNodes:
  enabled: true

# vCluster control plane options
controlPlane:
  distro:
    k8s:
      image:
        tag: v1.31.2 # Kubernetes version you want to use
  service:
    spec:
      type: LoadBalancer # If you want to expose vCluster via LoadBalancer (recommended option)

# Networking configuration
networking:
  # Specify the pod cidr
  podCIDR: 10.64.0.0/16
  # Specify the service cidr
  serviceCIDR: 10.128.0.0/16
```

:::info Connecting to the platform
Follow [this guide](https://www.vcluster.com/docs/vcluster/next/configure/vcluster-yaml/external/platform/api-key#connect-virtual-clusters-to-the-vcluster-platform) to create an access key and set up the vCluster for deployment using Helm or other tools.
:::

### Join worker nodes to the virtual cluster

*Pre-requisities on worker nodes*

- `curl` installed

In order to join worker nodes, a token from the vCluster must be created to provide access and permissions to join the vCluster. A single token can be used for any worker nodes to join, or if you wanted to, you could create a token for each node.

:::info Prerequisite
vCluster has to be running and ready.
:::

To join worker nodes, a token from the vCluster must be created to provide access and permissions to join the vCluster. A single token can be used for any worker nodes to join, or if you wanted to, you could create a token for each node.

```bash title="Create a token for worker nodes"
export VCLUSTER_NAME=my-vcluster

# Connect to your vcluster
vcluster connect $VCLUSTER_NAME

# Create a token
vcluster token create
```

Optional flags:
- `--expires=24h` - Set token expiration time. By default, it is set to 1 hour.

The output provides a command to run on your worker node:

```bash title="Example output from creating a token"
curl -sfLk https://<vcluster-endpoint>/node/join?token=<token> | sh -
```

this can be used with following flags:

| Flag | Description | Default |
| ---- | ----------- | ------- |
| `--kubernetes-version` | Version of kubernetes node components | optional, defaults to the version of vCluster k8s |
| `--repository-url` | Specific vCluster version to install | optional, defaults to `/usr/local/bin` |
| `--binaries-dir` | allows to customize target directory where k8s node components binaries are installed | optional, defaults to `/usr/local/bin |
| `--cni-binaries-dir` | allows to customize target directory where CNI binaries are installed | optional, defaults to `/opt/cni/bin` |
| `--skip-reset` | Skips uninstalling existing node components | optional, default to false (reset happens before install by default) |
| `--bundle-path` | If set, skips downloading Kubernetes binaries and uses a local path. Expects a path to bundle in `.tar.gz` format | optional, empty by default |
| `--reset-only` | only uninstall previously installed node components. Use this to cleanup the node | optional, defaults to `false` |
| `--skip-join` | Installs all required components but skips joining node to the cluster | optional, defaults to `false` |
| `--node-name` | Node name in the kubernetes cluster | optional, defaults to hostname |
| `--force-join` | Force joining even when there is existing kubelet service running on the node | optional, defaults to `false` |

you can pass these flags to the script like:
```bash title="Run join script with flags"
curl -sfLk https://<vcluster-endpoint>/node/join?token=<token> | sh -s -- --force-join
```

For each worker node that you want to join vCluster, run the command on the worker node.

```bash title="Example output on worker node"
Preparing node for Kubernetes installation...
Kubernetes version: v1.31.2
Installing Kubernetes binaries...
Enabling containerd and kubelet...
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /etc/systemd/system/containerd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /etc/systemd/system/kubelet.service.
Starting containerd and kubelet...
Installation successful!
Joining node into cluster...
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

## Load Docker images

To load a local Docker image on to a worker node you can use the following command:

```bash title="Load Docker image into specific node"
vcluster node load-image my-node --image nginx:latest
```

Node has to be in `Ready` state and able to run a pod.

This command pulls the docker image locally, saves it as a `.tar` archive, then uses `kubectl cp` to copy the image to the created pod.
Once image is copied to the pod, then it is imported to the node.

## Upgrade vCluster

### Upgrade vCluster control plane

:::warning Kubernetes Version Skew Policy applies
Do not upgrade multiple minor versions at the same time for the control plane as outlined in the [Kubernetes Version Skew Policy](https://kubernetes.io/releases/version-skew-policy/). Instead always upgrade a single minor version, wait until the cluster becomes healthy and then upgrade to the next minor version. For example: `v1.26` -> `v1.27` -> `v1.28`.
:::

To upgrade the vCluster control plane, update the Kubernetes `init` container version in your `vcluster.yaml` file.
After the change, vCluster upgrades the control plane automatically. If automatic worker node upgrades are configured, those nodes are also upgraded.

To change the Kubernetes version, add the following to your `vcluster.yaml`:

```yaml title="Upgrade control plane Kubernetes version"
...
controlPlane:
  statefulSet:
    image:
      tag: v1.31.1 # Or any other Kubernetes version
...
```

### Upgrade vCluster workers

:::warning Kubernetes Version Skew Policy applies
Use the [Kubernetes Version Skew Policy](https://kubernetes.io/releases/version-skew-policy/) for worker upgrades.
:::

There are two modes of how worker upgrades can be done:
- (_Recommended_) Automatically by vCluster when the control-plane was updated
- Manually using vCluster CLI or kubeadm

#### Use automatic upgrades

Automatic worker node upgrades are enabled by default. vCluster upgrades each node when it detects a version mismatch between the control plane and the worker node. By default, one node is upgraded at a time.
Upgrade starts 2 minutes after the control plane is upgraded. There is also a grace period for newly added worker nodes (upgrade is not going to start if node was created less than 2 minutes ago).

:::tip
You can exclude a node from automatic upgrade by labeling it with "vcluster.loft.sh/skip-auto-upgrade": "true"
:::

The upgrade Pod runs directly on the node and completes the following steps:
1. Download the Kubernetes binary bundle from the vCluster control plane
2. Replace the `kubeadm` binary
3. Run `kubeadm upgrade node`
4. Cordon the node
5. Replace other binaries such as `containerd`, `kubelet`, etc.
6. Restart `containerd` and `kubelet` if necessary
7. Uncordon the node

You can also skip specific nodes from the automatic update by adding the label `vcluster.loft.sh/skip-auto-upgrade=true` on the node or specifying a `nodeSelector` via the vCluster config.

<AutoUpgrade />

:::warning
Node upgrades typically do not restart pods. However, depending on the Kubernetes version change, restarts might occur in some cases.
:::

#### Perform manual upgrades

You can either upgrade a node using the vCluster CLI by running the following command:

```bash title="Upgrade a specific node manually"
vcluster node upgrade my-node
```

Alternatively, you can manually upgrade by following the [official Kubeadm Kubernetes guide](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/).

## Configure addons

vCluster supports addons that extend the functionality of your virtual cluster. You can configure these addons during deployment to adjust networking, observability, and other features for your environment and requirements.

### CNI

vCluster installs [Flannel](https://github.com/flannel-io/flannel) by default, if you want to install your own custom CNI, you can disable Flannel using the following:

```yaml title="Disable default Flannel CNI"
privateNodes:
  cni:
    flannel:
      enabled: false
```

### CoreDNS

vCluster installs [CoreDNS](https://coredns.io/) by default to allow cluster DNS resolving of services and pods.

### Konnectivity

vCluster uses [Konnectivity](https://kubernetes.io/docs/tasks/extend-kubernetes/setup-konnectivity/) to establish a connection between the control plane and worker nodes for commands such as `kubectl logs`, `kubectl exec`, `kubectl port-forward`. The Konnectivity server runs as part of the vCluster control plane, while an agent is deployed inside the vCluster. If it is not needed or wanted, you can disable Konnectivity in the vCluster config.

### Kube proxy

vCluster installs [Kube Proxy](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/) by default to ensure that services are configured on the nodes. Some CNI implement their own kube-proxy functionality. Optionally, you can disable kube-proxy deployment in the vCluster config.

### Local Path Provisioner

vCluster installs the [Local Path Provisioner](https://github.com/rancher/local-path-provisioner) by default to allow stateful workloads within the vCluster.

## Load Docker images into nodes

To load a local Docker image into a worker node you can use the following command:

```bash title="Load Docker image into specific node"
vcluster node load-image my-node --image nginx:latest
```

## Remove worker node

To remove a node from the cluster, follow standard procedures as with any other Kubernetes distro: first cordon a node, then drain it.
There is `vcluster` CLI helper command that does it:
1. Cordon and drain node:

```shell title="Node cordoning"
vcluster node delete <node-name>
```

3. Once no more workloads are running on the node, log into the node terminal and run the installation script that you used originally (more on this in [Install](#join-worker-nodes-to-the-virtual-cluster) section):
```shell title="reset node"
curl -sfLk https://<vcluster-endpoint>/node/join?token=<token> | sh -s -- --reset-only
```

4. Then, stop vCluster systemD service:

```shell title="stop vcluster.service"
systemctl stop vcluster.service
```

5. Now, you can safely remove service definition & other vCluster related files:
```shell title="remove vCluster files"
rm -rf /var/lib/vcluster && rm /etc/systemd/system/vcluster.service
```

## Reuse a worker node in another virtual cluster
Node can be reset and re-used in another virtual cluster.

To do this, follow steps from [Remove section](#remove-worker-node) first, and then [Join worker nodes to the virtual cluster](#join-worker-nodes-to-the-virtual-cluster). **Make sure to pass `--force-join` flag to the installation script**.

## Review configuration reference

<PrivateNodes />
<Networking />
<ControlPlane />
