---
title: Central Admission Control
sidebar_label: centralAdmission
sidebar_class_name: pro
sidebar_position: 5
description: Configuration for ...
---

import AdmissionControl from '@site/docs/_partials/config/policies/centralAdmission.mdx'
import ProAdmonition from '@site/docs/_partials/admonitions/pro-admonition.mdx'

<ProAdmonition/>

:::important
You must enable MutatingAdmissionWebhook and ValidatingAdmissionWebhook admission controllers to use the Central Admission Control feature. See the Kubernetes [Admission Controllers Reference](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-use) for details.
:::

Centralized Admission Control is an advanced feature for cluster admins that have custom rules that they need to apply to the virtual cluster. Cluster admins can enforce [Kubernetes admission webhooks](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) that reference the host cluster or external policy services from within the virtual cluster. Examples of validating and  mutating webhook based policy engines include OPA, Kyverno, and jsPolicy.

:::warning
- Central Admission Control is read-only after virtual cluster creation. Admins cannot bypass or alter hooks after vCluster deployment.
- If you need the ability to modify or delete webhook configurations, do not use Central Admission Control. Instead, manually map the host policy service into the virtual cluster.
:::

Central admission webhooks are different from the other policies:

- `LimitRange` and `ResourceQuota` resources are created and enforced on the host cluster. They do do not appear as resources in the virtual cluster.
- A user could define `LimitRange` and `ResourceQuota` resources inside the virtual cluster, but they have full control over them and can delete them at will.
- Webhooks are different because they need to be configured inside the virtual cluster in order for them to be called by the vCluster's API server.
- vCluster rewrites these definitions to point to a proxy for a host cluster service that handles webhook requests. The host may also have webhook configurations that use this service.
- A user can still install a webhook service or webhook configuration into the virtual cluster outside of this config, but it would run inside the virtual cluster like any other workload.

## Example

This example ensures that all new ConfigMap resources are sent to a mutating webhook on the host cluster.

```yaml
centralAdmission:
  mutatingWebhooks:
    - apiVersion: admissionregistration.k8s.io/v1
      kind: MutatingWebhookConfiguration
      metadata:
        labels:
          webhook.kyverno.io/managed-by: kyverno
        name: kyverno-resource-mutating-webhook-cfg
      webhooks:
      - admissionReviewVersions:
        - v1
        clientConfig:
          service:
            name: kyverno-svc
            namespace: kyverno
            path: /mutate/fail
            port: 443
        failurePolicy: Fail
        matchPolicy: Equivalent
        name: mutate.kyverno.svc-fail
        namespaceSelector:
          matchExpressions:
          - key: kubernetes.io/metadata.name
            operator: NotIn
            values:
            - kyverno
        objectSelector: {}
        reinvocationPolicy: IfNeeded
        rules:
        - apiGroups:
          - ""
          apiVersions:
          - v1
          operations:
          - CREATE
          resources:
          - configmaps
          scope: '*'
        sideEffects: NoneOnDryRun
        timeoutSeconds: 10
```

Any time a ConfigMap is created, the vCluster API server forwards the admission request to a proxy, which in turn calls the actual admission hook running in the host cluster in the `kyverno` namespace.

Note that the service providing the webhooks should not rely on the real names of the objects if they are synced on the host cluster (for example, pods). The requests that will reach the host admission service holds the objects from the virtual cluster, so your policies need to be able to handle those objects.

The hook services do not require authentication as the proxy does not have access to the `AdmissionConfiguration` object or the files it references. Most policy engines (like jsPolicy, Kyverno, or Gatekeeper) do not set authentication by default, so it should work with most installations.

If a user inside the virtual cluster (admin or not) tries to delete one of your admission hooks, that uses is prompted with the following error message:

```bash
kubectl delete kyverno-resource-mutating-webhook-cfg
error: the resource kyverno-resource-mutating-webhook-cfg is protected
```

## Config reference

<AdmissionControl/>
