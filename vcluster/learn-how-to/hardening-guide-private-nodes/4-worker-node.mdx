---
title: Self assessment guide - Worker Node Configuration
sidebar_label: Worker Node Configuration
description: Self assessment guide to validate Worker Node configuration
---

This section provides security recommendations for components running on Kubernetes worker nodes:
- Kubelet configuration and security
- File system permissions

_Assessment focus for vCluster_: Verification should focus on ensuring the private node meets these requirements.

<!-- vale Google.Headings = NO -->
## 4.1 Worker Node Configuration Files
<!-- vale Google.Headings = YES -->
### 4.1.1 Ensure that the kubelet service file permissions are set to 600 or more restrictive (Automated)

**Result:** PASS

**Remediation:**
Run the audit command to verify the permissions on the service file. If they do not match the expected result only then run the below command to set the appropriate permissions. 
```
chmod 600 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
```

**Audit:**
Run the following command against each node:
```bash
stat -c permissions=%a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
```
Verify that the permissions are 600 or more restrictive.

**Expected Result:** 
```
permissions has value 600, expected 600 or more restrictive
```

**Returned Value:**
```
permissions=600
```

### 4.1.2 Ensure that the kubelet service file ownership is set to root:root (Automated)

**Result:** PASS

**Audit:**
Run the following command against each node:
```bash
stat -c %U:%G /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
```
Verify that the ownership is set to root:root.

**Expected Result:** 
```
root:root
```

**Returned Value:**
```
root:root
```

### 4.1.3 If proxy kubeconfig file exists ensure permissions are set to 600 or more restrictive (Automated)

**Result:** PENDING

**Audit:**
Connect to the vCluster and run the following command to extract the kube-proxy pod name.
```bash
KUBE_PROXY_POD=$(kubectl get pods -n kube-system -l k8s-app=kube-proxy -o jsonpath='{.items[0].metadata.name}')
```

Save the below file for use with the debug container.
```yaml title="custom.json"
{
  "volumeMounts": [
    {
      "name": "kube-proxy",
      "mountPath": "/var/lib/kube-proxy"
    }
  ]
}
```

Run the following command against the kube-proxy pod:
```bash
kubectl debug --custom custom.json -it $KUBE_PROXY_POD --image=busybox --target=kube-proxy --namespace kube-system --profile=general -q -- stat -L -c permissions=%a /var/lib/kube-proxy/kubeconfig.conf
```
Verify that the permissions are 600 or more restrictive.

**Expected Result:** 
```
permissions has value 600, expected 600 or more restrictive
```

**Returned Value:**
```
permissions=600
```

### 4.1.4 If proxy kubeconfig file exists ensure ownership is set to root:root (Automated)

**Result:** PASS

**Audit:**
Connect to the vCluster and run the following command to extract the kube-proxy pod name.
```bash
KUBE_PROXY_POD=$(kubectl get pods -n kube-system -l k8s-app=kube-proxy -o jsonpath='{.items[0].metadata.name}')
```

Save the below file for use with the debug container.
```yaml title="custom.json"
{
  "volumeMounts": [
    {
      "name": "kube-proxy",
      "mountPath": "/var/lib/kube-proxy"
    }
  ]
}
```

Run the following command against the kube-proxy pod:
```bash
kubectl debug --custom custom.json -it $KUBE_PROXY_POD --image=busybox --target=kube-proxy --namespace kube-system --profile=general -q -- stat -c %U:%G /var/lib/kube-proxy/kubeconfig.conf
```
Verify that the ownership is set to root:root.

**Expected Result:** 
```
root:root
```

**Returned Value:**
```
root:root
```

### 4.1.5 Ensure that the --kubeconfig kubelet.conf file permissions are set to 600 or more restrictive (Automated)

**Result:** PASS

**Audit:**
Run the following command against each node:
```bash
stat -c permissions=%a /etc/kubernetes/kubelet.conf
```
Verify that the permissions are 600 or more restrictive.

**Expected Result:** 
```
permissions has value 600, expected 600 or more restrictive
```

**Returned Value:**
```
permissions=600
```

### 4.1.6 Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root (Automated)

**Result:** PASS

**Audit:**
Run the following command against each node:
```bash
stat -c %U:%G /etc/kubernetes/kubelet.conf
```
Verify that the ownership is set to root:root.

**Expected Result:** 
```
root:root
```

**Returned Value:**
```
root:root
```

### 4.1.7 Ensure that the certificate authorities file permissions are set to 600 or more restrictive (Automated)

**Result:** PASS

**Remediation:**
Run the audit command to verify the permissions on the certificate authorities file. If they do not match the expected result only then run the below command to set the appropriate permissions. 
```
chmod 600 /etc/kubernetes/pki/ca.crt
```

**Audit:**
Run the following command against each node:
```bash
stat -c permissions=%a /etc/kubernetes/pki/ca.crt
```
Verify that the permissions are 600 or more restrictive.

**Expected Result:** 
```
permissions has value 600, expected 600 or more restrictive
```

**Returned Value:**
```
permissions=600
```

### 4.1.8 Ensure that the client certificate authorities file ownership is set to root:root (Automated)

**Result:** PASS

**Audit:**
Run the following command against each node:
```bash
stat -c %U:%G /etc/kubernetes/pki/ca.crt
```
Verify that the ownership is set to root:root.

**Expected Result:** 
```
root:root
```

**Returned Value:**
```
root:root
```

### 4.1.9 Ensure that the kubelet --config configuration file has permissions set to 600 or more restrictive (Automated)

**Result:** PASS

**Remediation:**
Run the audit command to verify the permissions on the certificate authorities file. If they do not match the expected result only then run the below command to set the appropriate permissions. 
```
chmod 600 /var/lib/kubelet/config.yaml
```

**Audit:**
Run the following command against each node:
```bash
stat -c permissions=%a /var/lib/kubelet/config.yaml
```
Verify that the permissions are 600 or more restrictive.

**Expected Result:** 
```
permissions has value 600, expected 600 or more restrictive
```

**Returned Value:**
```
permissions=600
```

### 4.1.10 Ensure that the kubelet --config configuration file ownership is set to root:root (Automated)

**Result:** PASS

**Audit:**
Run the following command against each node:
```bash
stat -c %U:%G /var/lib/kubelet/config.yaml
```
Verify that the ownership is set to root:root.

**Expected Result:** 
```
root:root
```

**Returned Value:**
```
root:root
```

## 4.2 Kubelet
### 4.2.1 Ensure that the --anonymous-auth argument is set to false (Automated)

**Result:** PASS

**Audit:**
If using a Kubelet configuration file, check that there is an entry for authentication:anonymous: enabled set to false.
Run the following command against each node:
```bash
cat /var/lib/kubelet/config.yaml
```
Verify that the corresponding entry is set to false in the Kubelet config file.

**Expected Result:** 
```
...
authentication:
  anonymous:
    enabled: false
...
```

**Returned Value:**
```
...
authentication:
  anonymous:
    enabled: false
...
```

<!-- vale Google.Headings = NO -->
### 4.2.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Automated)
<!-- vale Google.Headings = YES -->

**Result:** PASS

**Audit:**
If using a Kubelet configuration file, check that there is an entry that sets authorization: mode to something other than AlwaysAllow.
Run the following command against each node:
```bash
cat /var/lib/kubelet/config.yaml
```
Verify that the corresponding entry is set to Webhook in the Kubelet config file.

**Expected Result:** 
```
...
authorization:
  mode: Webhook
...
```

**Returned Value:**
```
...
authorization:
  mode: Webhook
...
```

### 4.2.3 Ensure that the --client-ca-file argument is set as appropriate (Automated)

**Result:** PASS

**Audit:**
If using a Kubelet configuration file, check that there is an entry that sets authentication: x509: clientCAFile to the location of the client certificate authority file.
Run the following command against each node:
```bash
cat /var/lib/kubelet/config.yaml
```
Verify that the corresponding entry is set to the appropriate file in the Kubelet config file.

**Expected Result:** 
```
...
authentication:
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
...
```

**Returned Value:**
```
...
authentication:
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
...
```

<!-- vale Google.LyHyphens = NO -->
### 4.2.4 Ensure that the --read-only-port argument is set to 0 (Automated)
<!-- vale Google.LyHyphens = YES -->

**Result:** PASS

**Audit:**
Run the following command against each node:
```bash
ps -ef | grep kubelet
```
<!-- vale Google.LyHyphens = NO -->
Verify that --read-only-port is set to 0.
<!-- vale Google.LyHyphens = YES -->

**Expected Result:** 
```
--read-only-port=0
```

**Returned Value:**
```bash
root        1154       1  0 18:39 ?        00:00:00 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10 --pod-max-pids=1000000 --read-only-port=0 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
```

### 4.2.5 Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Manual)

**Result:** PASS

**Audit:**
If using a Kubelet configuration file, check that it does not set streamingConnectionIdleTimeout to 0.
Run the following command against each node:
```bash
cat /var/lib/kubelet/config.yaml
```
Verify that streamingConnectionIdleTimeout is not set to 0.

**Expected Result:** 
```
...
streamingConnectionIdleTimeout: 4h0m0s
...
```

**Returned Value:**
```
...
streamingConnectionIdleTimeout: 4h0m0s
...
```

### 4.2.6 Ensure that the --make-iptables-util-chains argument is set to true (Automated)

**Result:** PASS

**Audit:**
If using a Kubelet configuration file, check that it does not set makeIPTablesUtilChains to false.
Run the following command against each node:
```bash
cat /var/lib/kubelet/config.yaml
```
Verify that makeIPTablesUtilChains is not set to false.

**Expected Result:** 
```
...
makeIPTablesUtilChains: true
...
```

**Returned Value:**
```
...
makeIPTablesUtilChains: true
...
```

### 4.2.7 Ensure that the --hostname-override argument is not set (Manual)

**Result:** PASS

**Audit:**
Run the following command against each node:
```bash
cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
```
Verify that --hostname-override argument does not exist.

**Expected Result:** 
```
--hostname-override argument does not exist.
```

**Returned Value:**
```
--hostname-override argument does not exist.
```

### 4.2.8 Ensure that the eventRecordQPS argument is set to a level which ensures appropriate event capture (Manual)

**Result:** PASS

**Audit:**
If using a Kubelet configuration file, check that there is an entry to set eventRecordQPS: to an appropriate level.
Run the following command against each node:
```bash
cat /var/lib/kubelet/config.yaml
```
Verify that eventRecordQPS is set to an appropriate level for the cluster.

**Expected Result:** 
```
...
eventRecordQPS: 50
...
```

**Returned Value:**
```
...
eventRecordQPS: 50
...
```

### 4.2.9 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Automated)

**Result:** PENDING

**Remediation:** If using a Kubelet config file, edit the file to set tlsCertFile to the location of the certificate file to use to identify this Kubelet, and tlsPrivateKeyFile to the location of the corresponding private key file.
If using command line arguments, edit the kubelet service file  /etc/kubernetes/kubelet.conf` on each worker node and set the below parameters in `KUBELET_CERTIFICATE_ARGS` variable.
```bash
--tls-cert-file=<path/to/tls-certificate-file> --tls-private-key-file=<path/to/tls-key-file>
```
Based on your system, restart the kubelet service. For example:
```bash
systemctl daemon-reload
systemctl restart kubelet.service
```

### 4.2.10 Ensure that the --rotate-certificates argument is not set to false (Automated)

**Result:** PASS

**Audit:**
If using a Kubelet configuration file, check that there is an entry to set rotateCertificates: to true.
Run the following command against each node:
```bash
cat /var/lib/kubelet/config.yaml
```
Verify that rotateCertificates is set to true.

**Expected Result:** 
```
...
rotateCertificates: true
...
```

**Returned Value:**
```
...
rotateCertificates: true
...
```

<!-- vale Google.Headings = NO -->
### 4.2.11 Verify that the RotateKubeletServerCertificate argument is set to true (Automated)
<!-- vale Google.Headings = YES -->

**Result:** PENDING

**Audit:**
Ignore this check if serverTLSBootstrap is true in the kubelet config file or if the --rotate-server-certificates parameter is set on kubelet.
Run the following command against each node:
```bash
cat /var/lib/kubelet/config.yaml
```
Verify that rotateCertificates is set to true.

**Expected Result:** 
```
...
rotateCertificates: true
...
```

**Returned Value:**
```
...
rotateCertificates: true
...
```

<!-- vale Google.Headings = NO -->
### 4.2.12 Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers (Automated)
<!-- vale Google.Headings = YES -->

**Result:** PASS

**Audit:**
Run the following command against each node:
```bash
ps -ef | grep kubelet
```

Verify that the --tls-cipher-suites argument is set with one of the cipher suites listed below
```
TLS_AES_128_GCM_SHA256
TLS_AES_256_GCM_SHA384
TLS_CHACHA20_POLY1305_SHA256
TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA
TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA
TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305
TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256
TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA
TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA
TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA
TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
TLS_RSA_WITH_3DES_EDE_CBC_SHA
TLS_RSA_WITH_AES_128_CBC_SHA
TLS_RSA_WITH_AES_128_GCM_SHA256
TLS_RSA_WITH_AES_256_CBC_SHA
TLS_RSA_WITH_AES_256_GCM_SHA384
```

**Expected Result:** 
```
'--tls-cipher-suites' contains valid elements from 'TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256,TLS_RSA_WITH_3DES_EDE_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_256_GCM_SHA384'
```

**Returned Value:**
```bash
root        1154       1  0 18:39 ?        00:00:00 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10 --pod-max-pids=1000000 --read-only-port=0 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
```

<!-- vale Google.Headings = NO -->
### 4.2.13 Ensure that a limit is set on pod PIDs (Manual)
<!-- vale Google.Headings = YES -->

**Result:** PASS

**Audit:**
Run the following command against each node:
```bash
ps -ef | grep kubelet
```

Verify that --pod-max-pids is set correctly.

**Expected Result:** 
```
'--pod-max-pids' is set correctly
```

**Returned Value:**
```bash
root        1154       1  0 18:39 ?        00:00:00 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10 --pod-max-pids=1000000 --read-only-port=0 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
```

<!-- vale Google.Headings = NO -->
## 4.3 kube-proxy
<!-- vale Google.Headings = YES -->
### 4.3.1 Ensure that the kube-proxy metrics service is bound to localhost (Automated)

**Result:** PASS

**Audit:**
kube-proxy runs as a pod inside the vCluster and its configuration is backed by a ConfigMap named "kube-proxy" in the kube-system namespace.
Run the following command inside the virtual cluster:
```bash
kubectl get cm kube-proxy -n kube-system -o jsonpath='{.data.config\.conf}' | grep metricsBindAddress
```
Verify that metricsBindAddress is set to default.

**Expected Result:** 
```
metricsBindAddress: ""
```

**Returned Value:**
```
metricsBindAddress: ""
```