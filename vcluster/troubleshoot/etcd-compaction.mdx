---
title: Resolve etcd NOSPACE alarm in vCluster  
sidebar_label: etcd NOSPACE alarm  
sidebar_position: 1  
description: Diagnose and resolve the etcd NOSPACE alarm in vCluster.  
---

# Resolve etcd NOSPACE alarm in vCluster  

## Problem

The etcd `NOSPACE` alarm occurs when the etcd database in your vCluster runs out of disk space. This critical condition triggers health check failures that make your vCluster unusable and prevent any cluster actions. 

## Error message

You might find the following error in the logs of your etcd pods, if the etcd has run out of storage space: 

```bash title="etcd NOSPACE alarm"
etcdhttp/metrics.go:86 /health error ALARM NOSPACE status-code 503
```

<details id="etcd-nospace-alarm">
<summary>Identifying an etcd `NOSPACE` alarm in vCluster</summary>

When interacting with the affected vCluster using `kubectl`, API requests fail with timeout errors:

```bash
Error from server: etcdserver: request timed out
```

Additionally, the etcd health metrics endpoint returns a `503` status code and the following error:

```text
etcdhttp/metrics.go:86 /health error ALARM NOSPACE status-code 503
```

To verify the `NOSPACE` alarm, run the following command against the etcd instance:

```bash
etcdctl alarm list --endpoints=https://$ETCD_SRVNAME:2379 [...]
```

The output displays the triggered alarm:

```text
memberID:XXXXX alarm:NOSPACE
```
</details>

## Causes

The `NOSPACE` alarm occurs due to two common conditions:

- **Excessive etcd data growth:** A large number of objects—such as Deployments, ConfigMaps, and Secrets—can fill etcd’s storage if regular compaction is not performed.

- **Synchronization conflicts:** Conflicting objects between the vCluster and host cluster can trigger continuous sync loops. For example, a Custom Resource Definition (CRD) modified by the host cluster might sync back to the vCluster repeatedly. This behavior quickly fills etcd’s backend storage.

## Solution

To resolve the issue, compact and defragment the etcd database to free up space. Then, reconfigure etcd with automatic compaction and increase its storage quota to prevent recurrence.

### Step 1: Identify if there's a syncing conflict

Check for objects that might be caught in a sync loop:

```bash
kubectl -n <namespace> logs <vcluster-pod> | grep -i "sync" | grep -i "error"
```

If you find a problematic object, pause syncing for it in your vCluster config.

### Step 2: Compact and defragment etcd

1. Connect to each etcd pod:

   ```bash
   kubectl -n <namespace> exec -it <etcd-pod-name> -- sh
   ```

2. Set environment variable:

   ```bash
   export ETCD_SRVNAME=<etcd-pod-name>
   ```

3. Get current revision number:

   ```bash
   etcdctl endpoint status --write-out json \
       --endpoints=https://$ETCD_SRVNAME:2379 \
       --cacert=/run/config/pki/etcd-ca.crt \
       --key=/run/config/pki/etcd-peer.key \
       --cert=/run/config/pki/etcd-peer.crt
   ```

4. Compact etcd database:

   ```bash
   etcdctl --command-timeout=600s compact <revision-number> \
       --endpoints=https://$ETCD_SRVNAME:2379 \
       --cacert=/run/config/pki/etcd-ca.crt \
       --key=/run/config/pki/etcd-peer.key \
       --cert=/run/config/pki/etcd-peer.crt
   ```

5. Defragment etcd:

   ```bash
   etcdctl --command-timeout=600s defrag \
       --endpoints=https://$ETCD_SRVNAME:2379 \
       --cacert=/run/config/pki/etcd-ca.crt \
       --key=/run/config/pki/etcd-peer.key \
       --cert=/run/config/pki/etcd-peer.crt
   ```

6. Repeat for all etcd pods in your cluster.

### Step 3: Verify disk usage reduction

Check that the operation freed up space:

```bash
etcdctl endpoint status -w table \
    --endpoints=https://$ETCD_SRVNAME:2379 \
    --cacert=/run/config/pki/etcd-ca.crt \
    --key=/run/config/pki/etcd-peer.key \
    --cert=/run/config/pki/etcd-peer.crt
```

### Step 4: Disarm the NOSPACE alarm

Remove the alarm to restore normal operation:

```bash
etcdctl alarm disarm \
    --endpoints=https://$ETCD_SRVNAME:2379 \
    --cacert=/run/config/pki/etcd-ca.crt \
    --key=/run/config/pki/etcd-peer.key \
    --cert=/run/config/pki/etcd-peer.crt
```

## Prevention

Update your vCluster configuration to prevent future occurrences by implementing these recommended settings that enable automatic maintenance of your etcd database:

```yaml
controlPlane:
  backingStore:
    etcd:
      embedded:
        enabled: false
      deploy:
        enabled: true
        statefulSet:
          enabled: true
          extraArgs:
            - '--auto-compaction-mode=periodic'
            - '--auto-compaction-retention=30m'
            - '--quota-backend-bytes=8589934592'
```

This configuration enables periodic compaction every 30 minutes, sets etcd quota to 8GB (adjust based on your needs), and uses deployed etcd instead of embedded for better control.

## Verification

After implementing the solution:

1. Check that etcd pods are healthy:
   ```bash
   kubectl -n <namespace> get pods | grep etcd
   ```

2. Verify that vCluster is functioning properly:
   ```bash
   kubectl -n <namespace> get pods
   kubectl -n <namespace> logs <vcluster-pod> | grep -i "alarm"
   ```

## Best practices

To maintain optimal etcd performance in your vCluster environment, always monitor etcd disk usage with appropriate metrics collection, implement regular automated compaction schedules appropriate to your workload patterns, and size your etcd storage quota based on actual usage plus a comfortable buffer. Remember that proper etcd maintenance is essential for vCluster stability, especially in production environments with high object counts or frequent configuration changes.