---
title: Resolve ingress update conflicts with Rancher
sidebar_label: Ingress Conflicts
description: How to resolve ingress conflicts when deploying Rancher in both vCluster and the host cluster.
---

import Flow, { Step } from '@site/src/components/Flow';

# Resolve ingress update conflicts with Rancher

When deploying the Rancher agent in both vCluster and the host cluster with a single shared ingress, both agents repeatedly update the ingress object. This causes excessive load on the database and API server.

You might observe the following signs of ingress update conflicts:

- Endless updates to ingress resources in the host cluster
- Rapid growth of the API server database (may reach several GB)
- API server slowdown (simple commands like `kubectl get pods` take several seconds)

## Cause

This issue occurs due to a conflict between Rancher agents deployed in both vCluster and the host clusters. Both agents attempt to update annotations on the ingress using different naming conventions, creating a continuous update loop.

## Solution

This issue is resolved in vCluster version 0.19.8 and later. To fix the problem:

1. Upgrade your vCluster installation to version 0.19.8 or later
2
. If your API server is slow, compact your etcd database to reduce its size (specific to your Kubernetes distribution)

### Upgrade vCluster using Helm

<Flow id="upgrade-vcluster">
<Step>
**Check your current vCluster version**.

```bash
helm list -A | grep vcluster
```
</Step>

<Step>
**Upgrade to version 0.19.8 or later**.

If you installed vCluster using Helm, run the following command:

```bash
helm upgrade <mycluster> loft/vcluster --reuse-values --version 0.19.8
```

Replace `<mycluster>` with the name of your vCluster release.
</Step>

<Step>
**Verify the upgrade**.

```bash
helm list -A | grep vcluster
```

Confirm the version is now 0.19.8 or later.
</Step>
</Flow>

### Verify the fix

After upgrading, monitor your cluster to ensure:

- Ingress updates have stabilized
- The database size growth has slowed or stopped
- API server performance has returned to normal

:::note
You must [compact the etcd](./etcd-compaction) database before seeing performance improvements.
:::

### Check API server performance

Run a simple command and measure response time:

```bash
time kubectl get pods -A
```

The command should complete in under a second if the API server is performing normally.

## Additional resources

For more details about this fix and other changes in vCluster 0.19.8, refer to the [official release notes](https://github.com/loft-sh/vcluster/releases/tag/v0.19.8).