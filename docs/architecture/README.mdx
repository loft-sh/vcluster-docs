---
title: Architecture
sidebar_label: Architecture
sidebar_position: 5
---


Virtual clusters are Kubernetes clusters that run on top of other Kubernetes clusters. Compared to fully separate "real" cluster, a virtual cluster does not have its own node pools or networking. Instead, a virtual cluster has its own control plane that schedules workloads inside the host cluster. 



<figure>
  <img src="/docs/media/diagrams/vcluster-architecture.svg" alt="vcluster Pod Scheduling" />
  <figcaption>vCluster Architecture</figcaption>
</figure>

## Components

vCluster spins up a virtual cluster and has components to manage interaction between the virtual cluster and the host cluster. By default, vCluster runs as a single pod scheduled by a StatefulSet and consists of these containers:

- [Control plane](/architecture/control-plane.mdx)
- [Syncer](/architecture/syncer.mdx)

## Host cluster and namespace

Every vCluster instance runs on top of another Kubernetes cluster, called the host cluster. Each vCluster instance runs as a regular StatefulSet inside a namespace of the host cluster. This namespace is called the host namespace. Everything that you create inside the virtual cluster lives either inside the virtual cluster itself or inside the host namespace. 

It is possible to run multiple vCluster instances inside the same namespace. You can even run vCluster instances inside another vCluster (vCluster nesting).


## Kubernetes resources

The core idea of virtual clusters is to provision isolated Kubernetes control planes (API servers) that run on top of "real" Kubernetes clusters. When working with the virtual cluster's API server, resources first only exist in the virtual cluster. However, vCluster synchronizes some low-level Kubernetes resources to the host cluster.

### Purely virtual high-level resources

Generally, vCluster stores all the Kubernetes resource objects that you create using the vCluster API server in the data store of the vCluster instance. That applies in particular to all higher level Kubernetes resources, such as Deployments, StatefulSets, and CRDs. These objects only exist inside the virtual cluster and never reach the API server or data store (etcd) of the host cluster.

### Synchornized low-evel resources

To be able to actually start pods, vCluster synchronizes certain low-level resources, such as Pod and any ConfigMap objects mounts in pods, to the underlying host namespace so that the scheduler of the underlying host cluster can schedule these pods.


## Design principles

### Lightweight with low-overhead

vClusters should be as lightweight as possible to minimize resource overhead inside the underlying host cluster.

**Implementation:** This is mainly achieved by bundling vCluster inside a single Pod using K3s as a control plane.

### No performance degradation

Workloads running inside a virtual cluster (even inside nested vCluster instances) should run with the same performance as workloads that are running directly on the underlying host cluster. The computing power, the access to underlying persistent storage, and the network performance should not be degraded at all.

**Implementation:** This is mainly achieved by synchronizing pods, which means that the pods are actually being scheduled and started just like regular pods of the host cluste. Running a pod inside the virtual cluster and running the same pod directly on the host cluster is exactly the same in terms of computing power, storage access, and networking. 

### Reduce requests on the host cluster

vCluster should greatly reduce the number of requests to the Kubernetes API server of the underlying host cluster by ensuring that all high-level resources remain in the virtual cluster only, without ever reaching the host cluster.

**Implementation:** This is mainly achieved by using a separate API server that handles all requests to the virtual cluster and a separate data store that stores all objects inside the virtual cluster. The syncer component synchronizes only a few low-level resources to the host cluster and thus requires very few API server requests. All of this happens in an asynchronous, non-blocking fashion, which follows Kubernetes design principles.

### Flexible provisioning

vCluster should not make any assumptions about how users provision it. Users should be able to create vCluster instance on top of any Kubernetes cluster without requiring the installation of any server-side component to provision vCluster. Provisioning should be possible with any client-only deployment tool, such as the vCluster CLI, Helm, kubectl, or Kustomize. Users can add an operator or CRDs to manage vCluster instances (for example, using Argo to provision vCluster instances), but a server-side management plane should never be required for deploying vCluster.

**Implementation:** This is mainly achieved by making vCluster basically run as a simple StatefulSet + Service that can be deployed using any Kubernetes tool.

### No admin privileges required

To provision a vCluster instance, a user should never be required to have any cluster-wide permissions. If a user has the RBAC permissions to deploy a simple web app to a namespace, they should also be able to deploy vCluster to the namespace.

**Implementation:** This is mainly achieved by making vCluster basically run as a simple StatefulSet + Service. Typically every user has the privilege to run these if they have any Kubernetes access at all.

### Single namespace encapsulation

Each vCluster instance and all the workloads and data inside the virtual cluster should be encapsulated into a single namespace. Even if the virtual cluster has hundreds of namespaces, everything in the host cluster should be encapsulated into a single host namespace.

**Implementation:** This is mainly achieved by using a separate API server and data store. Additionally, the syncer component synchronizes everything to a single host namespace while renaming resources during the sync to prevent naming conflicts when mapping from multiple namespaces inside the virtual cluster to a single namespace in the host cluster.

### Easy cleanup

vCluster should not have any hard wiring with the underlying cluster. Deleting a vCluster or merely deleting the vCluster's host namespace should always be possible without any negative impacts on the underlying cluster (no namespaces stuck in terminating state or anything comparable). Also, deleting should always guarantee that all vCluster-related resources are being deleted cleanly and immediately without leaving any orphan resources behind.

**Implementation:** This is mainly achieved by not adding any control plane or server-side elements to the provisioning of a vCluster instance, which is just a StatefulSet and few other Kubernetes resources. All synchronized resources in the host namespace have an appropriate owner reference, which means if you delete the vCluster itself, everything that belongs to the vCluster is automatically deleted by Kubernetes as well. This is a similar mechanism to what a Deployment and StatefulSet use to clean up their pods.
